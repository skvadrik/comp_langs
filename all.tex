\documentclass[11pt]{book}
\usepackage{amsmath}
\usepackage[russian]{babel}
\usepackage{color}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}

% for nice symbols
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{wasysym}
\usepackage{marvosym}

\title{
\small{\textleaf}
\huge{\textleaf}
\huge{\quad}
\large{Сказ о том, чем компилятор от интерпретатора отличается :)}
\huge{\quad}
\huge{\textleaf}
\small{\textleaf}
}
\author{
\small{\textborn} \ 
\small{\APLstar} \ 
\huge{\textborn} \ 
\huge{\APLstar} \ 
\Huge{$\bigstar$} \ 
\huge{\APLstar} \ 
\huge{\textborn} \ 
\small{\APLstar} \ 
\small{\textborn}
\\
\Large (в вольном изложении братца)
\\
\huge{\Bicycle}
}
\date{май 2014}

\begin{document}

\maketitle
\pagebreak

\section*{Што это такое}
Программирование сильно смахивает на героическую борьбу человека с компьютером.
Человек пишет программу. Он пишет её на странном, нелогичном языке, с трудом выражая
свои мысли. Язык больше мешает, чем помогает: нужной функции нет, зато есть
куча каких-то дурацких никому не нужных штук.
\\ \\
Вот человек написал программу. Посидел, порассматривал своё кривоватое детище.
С виду --- странная смесь английских слов и бессмысленных крикозябликов.
Но цепкие глаза программиста смотрят сквозь буквы, въедаются
в неуловимую суть. Наконец человек устаёт пыриться в экран и отдаёт программу
на суд компьютера. Начинается долгое, утомительное противостояние: человек
то букву добавит, то две уберёт, то вообще начинает удалять текст большими кусками.
\\ \\
И вот наступил великий момент: программа запустилась. На чёрном фоне
высветились белые буквы ``Segmentation fault''.
\\ \\
Что я хочу сказать этой болтовнёй, так это то, что языки программирования малость не идеальны. :D
Я хотела рассказать тебе, что я о них знаю, что меня вдохновляет
и позволяет худо-бедно продираться через сотни костылей и заведомую плохость результата.
\\ \\
План у меня примерно такой.
Я начну с мутных рассуждений о том, что такое язык программирования.
Потом поговорю о более конкретных вещах: что такое машинный язык и как от него перейти к языкам высокого уровня.
Наконец, верхом конкретности будет пример построения нескольких языковых процессоров для очень простого языка.
\\ \\
Я много где могу привирать или говорить не всю правду, и наверняка даже не про все такие случаи догадываюсь.
Зато я пыталась взять шириной охвата. :D
Скажем так, я пыталась разбросать щупальца во все стороны, но при этом в паре мест въедливо ковыряюсь в мелочах
(чтобы показать, что магии нет, и в любом программистском вопросе можно докопаться до сути).
\\ \\
Поковыряй пример, пойми, как он работает. :)
\\ \\
Есть много книжек, которые стоит прочитать.
Не для того, чтобы стать очень умной --- это мелкий побочый эффект.
Просто программирование без идеи --- на редкость нудное дело, а идея как фонарь --- то потухнет, то погаснет.
Книжки, то есть мысли других людей, не дадут твоей идее погаснуть.
Я их люблю за это, а умность --- хоть бы её и совсем не было.
\\ \\
Из совсем уж классных книжек:
Petzold, ``Code'' (про устройство компьютера);
Tanenbaum (он много про что написал: архитектура компьютеров, компьютерные сети);
SICP (Structure and Interpretation of Computer Program, я лично не читала, но собираюсь, хорошие люди советуют);
Jacobs, Grune: ``Parsing Techniques. A Practical Guide'', 2nd edition (моя лично горячо любимая книжка про синтаксический анализ);
Kline: ``Mathematics: the Loss of Certainty'' (про историю и смысл математики).
И другие...

\tableofcontents



\chapter{Малость о языках программирования}
Язык программрования определяется двумя вещами: синтаксисом и семантикой.
Синтаксис --- это что на языке можно сказать.
Семантика --- это как следует понимать сказанное.
Синтаксис подобен форме, семантика --- сути.
\\ \\
По-хорошему, и синтаксис, и семантика должны быть \emph{формально} определены.
Формально определённый язык представляет из себя чистую абстракцию, он не привязан к конкретной реализации.
Это позволяет любому желающему написать свою реализацию языка,
что хорошо как для программистов (есть выбор),
так и для разработчиков языка (необходимость удовлетворять формальному определению неслабо дисциплинирует,
а здоровая конкуренция реализаций постоянно подстёгивает).
\\ \\
На практике встречаются разные языки.
У некоторых вообще нет формального определения --- только мутное описание и единственная полусгнившая реализация.
Другие худо-бедно дают формальное определение синтаксиса, обычно вперемешку с замечаниями и комментариями.
С семантикой дело обстоит хуже: хорошо, если она просто описана словами и обрывками псевдокода.
\\ \\
Попытка формально описать язык называется \emph{стандартом} языка.
Название стандарта часто отражает год его принятия (C99, C++11, Haskell98)
или стандартизирующую организацию (ECMA-262 --- стандарт JavaScript, ECMA-408 --- стандарт Dart).

\section{Синтаксис}
Хотя синтаксис --- всего лишь форма, он очень важен как для программистов (им в этой форме выражать свои мысли),
так и для разработчиков языкового процессора (им писать синтаксический анализатор).
\\ \\
С точки зрения программиста в синтаксисе важны:
\begin{itemize}
\item Простота --- она позволяет программисту сосредоточиться на решаемой задаче.
Кроме того, простой синтаксис легко изучить, и язык будет привлекать, а не отпугивать новых программистов.
\item Минималистичность --- чем меньше кода требуется для решения задачи, тем легче охватить его одним глазом и понять.
\item Ортогональность --- хорошо, когда семантически разные конструкции описываются визуально непохожими закорючками.
\item Однозначность --- плохо, когда есть много семантически эквивалентных способов выразить мысль, это приводит к путанице и разнобою
(тут, впрочем, многие любители разнообразия со мной не согласятся).
\item Красота --- работа программистов нематериальна, а результат далёк и отвязан от жизни,
поэтому важно видеть в ней хоть какую-то простую человеческую ценность.
\end{itemize}
С точки зрения разработчика языкового процессора важно:
\begin{itemize}
\item Синтаксическая простота --- то есть возможность алгоритмически легко распознавать синтаксис на компьютере.
\item Расширяемость --- возможность легко добавлять в язык новое или изменять старое.
В этом смысле разработчики синтаксиса должны предвидеть будущее, оставлять свободу для расширения.
\end{itemize}
Дальше мы будем говорить о синтаксисе с точки зрения разработчика языка,
то есть обсуждать подходы к формальному определению и алгоритмам распознавания синтаксиса.

\subsection{Формальные грамматики}
Где-то в районе 2-й половины XX века
(в связи с распространением компьютеров и рождением языков программирования)
человечество придумало универсальный математический аппарат для описания синтаксиса языка: \emph{формальные грамматики}.
\\ \\
Формальная грамматика --- это четвёрка $G=<\Sigma_T,\Sigma_N,P,S>$, где:
\begin{enumerate}
\item $\Sigma_T$ --- множество терминалов (алфавит);
\item $\Sigma_N$ --- множество нетерминалов;
\item $P$ --- множество правил вывода вида $\alpha A \beta \rightarrow \gamma$,
где $\alpha, \beta, \gamma \in (\Sigma_T \cup \Sigma_N)^*$ ($\Sigma^*$ означает множество всех цепочек в алфавите $\Sigma$),
а $A \in \Sigma_N$;
\item $S \in \Sigma_N$ --- стартовый нетерминал.
\end{enumerate}
Множество всех цепочек, выводимых из стартового нетерминала и состоящих только из терминалов, называется \emph{языком},
порождаемым грамматикой.
(Говоря о формальных грамматиках, язык отождествляют с его синтаксисом).
Грамматика однозначно определяет язык, но один и тот же язык может быть порождён разными грамматиками.
\\ \\
Классификацию грамматик (и языков соответственно) обычно проводят по виду правил вывода.
Самая известная классификация --- классификация Ноама Хомского (1956 год):
\begin{enumerate}
\item Type 0 (unrestricted, неограниченные): правила вида $\alpha A \beta \rightarrow \gamma$
\item Type 1 (context-sensitive, контекстно-зависимые): правила вида $\alpha A \beta \rightarrow \alpha \gamma \beta$ ($\gamma$ --- не пустая строка)
\item Type 2 (context-free, контекстно-свободные): правила вида $A \rightarrow \gamma$
\item Type 3 (regular, регулярные): правила вида $A \rightarrow \alpha B$ или $A \rightarrow \alpha$
\end{enumerate}
Каждый следующий класс, как ты могла заметить по виду правил, --- сужение предыдущего, (то есть более простой, ограниченный).
Обсудим эти четыре класса, начиная с самого простого и заканчивая самым сложным.

\subsubsection{Регулярные языки и регулярные выражения}
Регулярные языки --- самые простые.
Проще них может быть только язык, задаваемый простым перечислением всех слов этого языка.
Типичный пример регулярной грамматики --- грамматика для описания натуральных десятичных чисел
($|$ --- это метасимвол, обозначающий ``или'': он разделяет альтернативные правые части правила):

$$\Sigma_T=\{0,1,2,3,4,5,6,7,8,9\}$$
$$\Sigma_N=\{digit,nonzero\_digit,number\}$$
$$P=\left\{
\begin{array}{l l}
digit & \rightarrow 0|1|2|3|4|5|6|7|8|9
\\
nonzero\_digit & \rightarrow 1|2|3|4|5|6|7|8|9
\\
number & \rightarrow number \ digit \ | \ nonzero\_digit
\end{array}
\right. $$
$$S=number$$
\\
Регулярные выражения --- это другой подход к описанию того же самого понятия.
Регулярные выражения над алфавитом $\Sigma$ определяются так:
\begin{itemize}
\item $\emptyset$ (пустое множество) --- регулярное выражение;
\item $\epsilon$ (пустая строка) --- регулярное выражение;
\item $\alpha \in \Sigma$ (символ из алфавита) --- регулярное выражение;
\item если $e_1$ и $e_2$ --- регулярные выражения, то $e_1 \  e_2$ (конкатенация) --- регулярное выражение;
\item если $e_1$ и $e_2$ --- регулярные выражения, то $e_1 | e_2$ (альтернатива) --- регулярное выражение;
\item если $e$ --- регулярное выражение, то $e^*$ (итерация, или звезда Клини) --- регулярное выражение;
\end{itemize}
С помощью регулярных выражений натуральные десятичные числа можно описать так:
$$number = (1|2|3|4|5|6|7|8|9)\ (0|1|2|3|4|5|6|7|8|9)*$$
Регулярные выражения и регулярные грамматики эквивалентны по выразительной силе:
они описывают одни и те же языки (регулярные).
Есть алгоритм, позволяющий для любой регулярной грамматики построить эквивалентное регулярное выражение, и наоборот.
\\ \\
Регулярные выражения --- очень полезная в хозяйстве вещь.
Представь, например, ситуацию: в большом текстовом файле нужно найти все строки, начинающиеся с определённого слова,
или все слова, в которых $N$ букв, причём половина букв --- согласные, или ещё какое-нибудь дурацкое ограничение.
В программах часто возникают подобные задачи, поэтому почти у каждого языка программирования есть библиотека регулярных выражений
(а во многие языки они попросту встроены).
Но ещё чаще регулярные выражения пригождаются в жизни: например, когда на файловой системе нужно быстренько найти все файлы с именем определённого вида,
или все файлы, в которых есть строка определённого вида, или отфильтровать вывод слишком болтливой программы.
В линуксе есть много удобных и жутко полезных консольных программ для этого
(посмотри хотя бы на \texttt{grep}, \texttt{sed} и \texttt{F7} или \texttt{F4} в \texttt{mc} --- зуб даю, не пожалеешь!).
\\ \\
Ввиду своей невероятной полезности регулярные выражения обросли целым рядом довесков и наворотов.
Большинство из этих наворотов --- синтаксический сахар, то есть просто более краткая и удобная запись базового синтаксиса.
Вот некоторые из них:
$e^+$ (одно или более повторений),
$e?$ (ноль или одно повторение),
$[a_i-a_j]$ (класс символов от $a_i$ до $a_j$),
$e\{n\}$ (ровно $n$ повторений),
$e\{n,m\}$ (от $n$ до $m$ повторений).
Все эти конструкции выражаются через базовые (например, $e^+$ --- это $ee^*$).
Но есть и другие, принципиальные навороты, из-за которых регулярные выражения перестают быть регулярными
(они начинают описывать куда более широкий класс языков).
Самый главный такой наворот --- обратные ссылки (backreferences).
Это конструкция, позволяющая выражению ссылаться на часть самого себя:
например, $(cat|dog)\textbackslash1$ описывает язык $\{catcat,dogdog\}$.
Подлинные регулярные выражения отличаются от таких ``расширенных'' не-регулярных выражений тем,
что первые можно обработать эффективно (алгоритмическая сложность --- $O(n)$, линейная),
а вторые --- нельзя (алгоритмическая сложность --- $O(2^n)$, экспоненциальная).
И те, и другие бывают полезны, но в разных задачах.
\\ \\
В последнее время возникла большая путаница, что считать регулярными выражениями.
Одни подразумевают регулярность в математическом смысле (допускают только синтаксический сахар).
Другие подразумевают наличие обратных ссылок и прочих не-регулярных наворотов.
Эта путаница осложняется наличием множества мутных ``стандартов'' типа
``Basic Regular Expressions'', ``Extended Regular Expressions'', ``Perl Regular Expressions'', ``Posix Regular Expressions'' и т.д.
Большинство людей путает эти понятия, и в каждом конкретном случае нужно выяснять, что имеется в виду.
\\ \\
Другое важное (и даже основное) применение регулярных языков --- \emph{лексический анализ}
(так называется синтаксический анализ регулярных языков).
Часто грамматику языка бывает удобно разбить на два уровня: лексическая грамматика и синтаксическая грамматика.
Такое разделение возникает, когда текст (поток терминальных символов) распадается на чёткие отдельные группы символов --- лексемы.
В естественных языках это могут быть слова и пунктуация, в языках программирования --- идентификаторы, литералы, операторы и прочее.
Сам текст при этом удобно воспринимать не как поток символов, а как поток лексем.
Структура лексем описывается лексической грамматикой языка, а взаимосвязи между лексемами --- синтаксической грамматикой.
Обычно лексическая грамматика простая (регулярная), а синтаксическая --- как повезёт (хорошо, если контекстно-свободная).
\\ \\
Регуляные языки отлично подходят для описания простых вещей: чисел, имён, IP-шников, URL-ов, ассемблерных инструкций.
Они прекрасно выражают последовательность, альтернативу и повторение.
Но для описания сложных связей и закономерностей они не подходят.
Самое яркое, бросающееся в глаза ограничение регулярных языков --- их неспособность выражать вложенные структуры
(без которых не обходится большинство высокоуровневых языков программирования).
Простейший пример --- правильные скобочные выражения.
\\ \\
Как уже говорилось, алгоритмическая сложность лексического анализа --- $O(n)$, где $n$ --- длина строки.
Математический аппарат для лексического анализа --- детерминированные конечные автоматы (Deterministic Finite Automata, DFA).

\subsubsection{Контекстно-свободные языки и нотация Бэкуса-Наура}
Следующий по сложности класс языков --- контекстно-свободные.
Это очень важный класс: он используется для синтаксического анализа большинства языков программирования.
Например, пресловутые правильные скобочные выражения описываются так:

$$\Sigma_T=\{(,)\}$$
$$\Sigma_N=\{A\}$$
$$P=\left\{
\begin{array}{l l}
A & \rightarrow ( A )\ |\ A A\ |\ ()
\end{array}
\right. $$
$$S=A$$
\\
Для описания контекстно-свободных грамматик часто используется нотация Бэкуса-Наура (BNF),
или расширенная нотация Бэкуса-Наура (Extended BNF, EBNF).
Она мало отличается от обычных правил вывода.
\\ \\
Алгоритмическая сложность синтаксического анализа контекстно-свободных языков --- $O(n^3)$,
детерминированных контекстно-свободных языков --- $O(n)$.
Математический аппарат --- автоматы с стековой памятью (PushDown Automata, PDA).
\\ \\
Как следует из названия (и из вида правил вывода), контекстно-свободные языки не позволяют выражать зависимость от контекста.
Простейший пример не контекстно-свободного языка --- язык $L=\{a^n b^n c^n, n >= 1\}$.
Более близкий к жизни пример --- фрагмент программы на языке C: ``\texttt{a * b;}''.
В зависимости от того, что есть \texttt{a} и \texttt{b}, это может означать объявление указателя или мультипликативное выражение.
\\ \\
В интернедах часто возникают споры о том, является ли грамматика того или иного языка программирования контекстно-свободной.
В этих спорах обычно доказывается, что мол, нет, не является, и зря бедным школьникам из года в год втирают о полезности контекстно-свободных грамматик.
Формально эти люди правы: если рассматривать полную, исчерпывающую грамматику языка программирования,
то почти наверняка она будет не контекстно-свободной.
Но важно другое: часто грамматику можно \emph{приблизить} с помощью контекстно-свободной и использовать эффективный алгоритм анализа.
Чтобы выразить не контекстно-свободные детали, используются стандартные ухищрения:
грамматику делят на лексическую и синтаксическую,
используют глобальную таблицу символов (в примере ``\texttt{a * b;}'' это позволяет установить тип \texttt{a} и \texttt{b}).
В общем, хотя языки программирования не контекстно-свободные, анализируют их контекстно-свободными методами.

\subsubsection{Контекстно-зависимые языки}
Следующий класс языков --- контекстно-зависимые.
Эти языки уже настолько сложные, что в общем случае алгоритмическая сложность их синтаксического анализа --- $O(2^n)$ (экспоненциальная).
Экспоненциальная сложность --- это очень плохо.
Это значит, что удлиннение программы на один символ может в разы увеличить время её анализа.
Из-за такой чудовищной сложности никто не станет работать с контекстно-зависимыми языками в чистом виде:
обычно работают с более простыми контекстно-зависимыми подклассами.
\\ \\
Математический аппарат для анализа контекстно-зависимых языков --- машина Тьюринга с ограниченной памятью (Linear Bounded Automaton, LBA).
(О машинах Тьюринга я поговорю чуть позже.)

\subsubsection{Неограниченные языки}
Ну и, наконец, самый широкий класс языков --- неограниченные.
В общем случае задача синтаксического анализа этих языков алгоритмически неразрешима (undecidable).
Вот такие они сложные, хо-хо!
На самом деле, в математике же как: поставил задачу в самом общем виде, а она возьми да и окажись эквивалентной задаче останова (Halting Problem).
Всё упирается в один и тот же потолок.
Математический аппарат для анализа неограниченных языков --- не кто иной, как сама великая и ужасная машина Тьюринга.
\\ \\
Любопытно, что долгое время шли споры о принадлежности естественных языков (в частности, английского) к конкретному классу.
Лично мне интуиция говорит, что английский конечно же не контекстно-свободный, и скорее всего даже не контекстно-зависимый, а неограниченный.
Было много ``наивных'' доказательств того, что английский не контекстно-свободный,
но строгое формальное доказательство было получено сравнительно недавно.
Дальше дело вроде бы не пошло: никто ещё не доказал неограниченности грамматики английского (но и не построил контекстно-зависимую грамматику).
\\ \\
Ещё любопытнее попытаться выйти за пределы логики первого порядка и посмотреть на какие-нибудь более широкие классы языков.
Здесь, впрочем, кончается школьная математика и начинаются куда более странные и философские вещи.
О них написано в книге Морриса Клайна ``Mathematics: The Loss of Certainty''.
Эту книжку прочитай --- хоть убейся, мало я \emph{таких} книжек знаю.
Можешь, правда, не сразу, а как время буит. ;D
\\ \\
Моя любимая книжка о синтаксическом анализе --- Jacobs, Grune: ``Parsing Techniques. A Practical Guide'' (2nd edition).
Это книжка невероятной широты: она охватывает сотни алгоритмов, затонувших статей в интернете, покрытых пылью диссертаций и горящих прорывов.
С другой стороны, это книжка философская, и рассчитана на широкий круг читателей: лингвистов, математиков, программистов.
Прочитай там первые пару глав (это быстро). :)
\\ \\
Другая книжка о синтаксическом анализе и компиляции в целом --- Aho, Sethi, Lam, Ullman: ``Compilers: Principles, Techniques, and Tools''.
Она считается классической книжкой для разработчиков компиляторов.
Просмотри там первые главы (это быстро). :)

\section{Семантика}
Семантика языка определяет смысл синтаксиса.
Задана она может быть по-разному: от простого словесного описания до чёткого алгоритма какой-нибудь абстрактной математической машины.
В любом случае семантика определяет \emph{выразительную силу} языка, его способность решать разные задачи.
\\ \\
Влияние семантики на жизнь программиста не такое явное, как влияние синтаксиса, но куда более серьёзное.
Программисту важны следующие качества семантики:
\begin{itemize}
\item Выразительность --- если семантика не позволяет решить задачу, или позволяет со скрипом, придётся поменять язык.
\item Простота --- чтобы писать программы, достаточно знания синтаксиса,
а вот семантику программистам изучать обычно лень (но приходится, когда в программе что-то идёт не так).
Поэтому очень важно, когда взмыленный, доведённый до отчаяния программист таки полезет читать ман,
чтобы в этом мане всё было просто, логично и без подковёрных мелких деталей и оговорок.
\item Надёжность --- семантика не должна меняться вообще никогда (или крайне редко),
иначе программы будут ломаться самым худшим образом (хитро и незаметно).
\end{itemize}
Разработчику языка важно:
\begin{itemize}
\item Соответствие семантики чёткой математической модели вычислений --- разработчик должен хорошо представлять вычислительные возможности языка,
иначе он не сможет реализовать язык вообще, или реализует его неэффективно.
\item Расширяемость --- часто, чтобы расширить выразительную силу языка, к нему прикручивают какие-то сбоку-бантики,
которые ломают вычислительную модель языка и сбивают с толку программистов.
Разработчик языка должен предвидеть эти сбоку-бантики и заранее заложить достаточно выразительную модель вычислений.
\end{itemize}

\subsection{Виды семантики}
Я выделю два основных вида семантики:
\begin{itemize}
\item Денотационная семантика определяет, \emph{что} означает синтаксическая конструкция на данном языке
в терминах другого языка (обычно какой-нибудь абстрактной математической системы),
то есть \emph{транслирует} программу.
\item Операционная семантика говорит, \emph{как} вычислить значение синтаксической конструкции на данном языке,
то есть \emph{интерпретирует} программу.
Алгоритм вычисления подразумевает исполнителя, в качестве которого может выступать человек, компьютер, абстрактная машина или ещё кто-то.
\end{itemize}
Обычно выделяют ещё аксиоматическую семантику: она определяет смысл синтаксической конструкции языка косвенно,
через набор логических аксиом и утверждений, истинность или ложность которого надо установить.
Но я почему-то не могу выделить её в отдельный вид: мне она кажется подвидом денотационной семантики,
просто таким вот ``особенным''.
А ты --- смотри сама, лучше почитай умные книжки. ;)
\\ \\
То, что денотационная семантика транслирует, а операционная --- интерпретирует,
не означает, что в первом случае у языка должен быть транслятор, а во втором --- интерпретатор.
Определение любого вида семантики апеллирует к каким-то аксиомам: денотационной --- к другому языку;
операционной --- к командам исполнителя; аксиоматической --- к законам логики.
Семантика может быть смешанной.

\subsection{Выразительная сила языка}
Разные языки предназначены для разных задач.
Грубо языки можно поделить на \emph{специализированные} и \emph{общего назначения},
но это разделение скорее относится с сфере использования языка, а не к его выразительной силе.
Выразительная сила --- это то, насколько широкий круг задач можно решать на этом языке.
\\ \\
Когда начинаешь об этом думать, возникает много вопросов:
\begin{itemize}
\item Насколько сложные бывают задачи?
\item Все ли задачи можно в принципе решить?
\item Если нет, можно ли по задаче хотя бы понять, решаемая она или нет?
\item Можно ли быть уверенным, что задача решаема, не зная её решения?
\end{itemize}
Вопросы непростые, и ответы на них вросли корнями в историю математики.
Про всё это хорошо написано в книге Морриса Клайна ``Mathematics: The Loss of Certainty'',
и кое-какие мысли из этой книги (видимо, грязно перевранные) я щас приведу.
\\ \\
Первыми по-настоящему заинтересовались математикой древние греки.
Математика существовала и раньше, но египтяне использовли в основном отдельные её элементы и в чисто практических целях.
Греки увидели в математике внутреннюю силу и красоту, средство идеально описывать законы реального мира.
Они придумали аксиоматический подход и основали на нём геометрию.
С алгеброй было сложнее: почти сразу вылезли иррациональные и отрицательные числа,
для которых не было аналогии в реальном мире, но математические законы почему-то выполнялись.
Возникали горячие споры, что делать с такими ``искусственными'' областями математики:
объяснить не получалось, а выкинуть не хотелось (слишком уж полезные, тем более что среди математиков было немало физиков, астрономов и инженеров).
\\ \\
Следом за греками были индусы --- их математика не очень интересовала с философской точки зрения, больше с практической,
поэтому они смело и решительно использовали ``искусственные'' области математики.
Индусы придумали много интересных вещей (в том числе --- ноль), но ничего толком не объяснили.
\\ \\
Потом были арабы.
Они внимательно изучили геометрию древних греков (и исправили некоторые неточности и ошибки в доказательствах теорем)
и сильно продвинули алгебру, которая у древних греков считалась второстепенной по сравнению с геометрией.
Но арабы тоже не очень интересовались основами математики и связанными с ними философскими проблемами.
\\ \\
Зато эти проблемы заинтересовали европейцев.
Математика дошла до них в интересном виде:
от древних греков осталась евклидова геометрия --- свод незыблемых истин о природе, основанных на малом числе аксиом;
от индусов и арабов --- ни на чём не основанная, но крайне полезная алгебра.
Математиков беспокоило, что необъяснёнными остаются самые простые и фундаментальные вещи: числа.
А тем временем математика развивалась стремительными темпами: новые теории были позарез нужны физикам и астрономам.
Кроме иррациональных и отрицательных чисел, появилось много новых ``искусственных'' понятий: дифференциалы, интегралы, комплексные числа и матрицы.
Всё чаще математики замечали, что основная беда кроется в понятии бесконечности:
как только начинаешь работать с бесконечностью как с числом, законы ломаются.
Они даже стали различать два понятия бесконечности: \emph{потенциальная бесконечность} --- это когда рассматриваются произвольно маленькие или большие величины;
и \emph{реальная бесконечность} --- это когда бесконечность воспринимается как нечто целое, единое.
\\ \\
Между тем, как алгебра считалась ``искусственной'' (или как минимум необоснованной), в абсолютности геометрии не было сомнений.
Долгое время люди верили, что геометрия --- это язык Бога, с помощью которого он идеально точно описывает законы природы.
Аксиомы Евклида абсолютны, потому что они следуют из законов реального мира.
Геометрия, построенная на этих аксиомах --- единственно возможная геометрия нашего мира.
В её доказательствах могут быть мелкие ошибки, но все они исправимы, потому что основа незыблема, основа --- это сама природа.
\\ \\
Но ведь если не верить в Бога, то нет особых оснований верить ни во что абсолютное, в том числе в аксиомы Евклида.
Когда люди задумались об этом, они поняли, что геометрия не основана ни на чём, кроме чистой веры.
Эта мысль не давала им покоя.
Математики и философы пытались доказать, что аксиомы следуют из чего-то ``более объективного'':
законов природы, интуиции, чувств, каких-то человеческих особенностей восприятия реальности.
Ни одно из этих объяснений не было удовлетворительным.
Хуже того: оказалось, что вместо евклидовых аксиом можно взять другие и построить геометрию, которая тоже будет описывать реальный мир.
\\ \\
Пока математики свыкались с мыслью о неабсолютности евклидовой геометрии, похожие открытия продолжались в других областях.
Неабсолютность обнаруживалась во всём: аксиомы, методы доказательства, даже предназначение математики было предметом горячих споров.
Оказалось, что математика никак не следует из природы, скорее из человеческой головы.
А голова --- она у всех разная.
\\ \\
Математики раскололись на несколько школ: логики, интуиницонисты, формалисты, теоретико-множественники.
Каждая школа предлагала свои основы и свои методы доказательства.
Математики разных школ предприянли героические усилия в попытках построить математику заново,
и во многом преуспели: дыры были залатаны, основы положены.
Но как раз когда почти всё уже было готово, на них обрушились настоящие трагедии.
\\ \\
В математике обнаружились странные противоречия --- парадоксы.
Непонятно было, откуда они берутся, и главное --- можно ли перестроить математику так, чтобы их не было.
В 1930 был получен результат, навсегда разрушивший веру в абсолютность.
Это теорема Гёделя о неполноте: если формальная система, достаточно выразительная для целых чисел, непротиворечива --- она неполна.
Это грубая, неточная формулировка, но идею она передают: какой набор аксиом ни выбери, как ни построй на них математику,
всё равно в ней останутся неразрешимые задачи или противоречия.
\\ \\
В свете этого немного по другому выглядит вопрос о разрешимости задачи.
Чтобы задачу можно было решать, она должна быть формально сформулирована.
Формулировка привязывает задачу к какой-то формальной системе, и решение задачи должно выражаться в терминах этой же системы.
Но мы знаем, что в любой непротиворечивой формальной системе есть неразрешимые задачи.
Значит, нет гарантии, что наша задача не окажется одной из них.
Более того, нельзя заранее выяснить вопрос о разрешимости: если повезёт, мы решим задачу;
если не очень повезёт --- докажем неразрешимость; а если совсем не повезёт --- вообще ничего не докажем.
\\ \\
У математика-теоретика после этого опускаются руки.
К счастью, (как сказал какой-то хороший человек) ``в реальности всё не так, как на самом деле''.
Да, никакая математическая система не может похвастаться разрешимостью любой задачи.
Но это не значит, что все они одинаково плохи: одни позволяют решить меньше задач, другие --- больше.
Даже если плюнуть на неразрешимые задачи, для многих разрешимых нету достаточно эффективного алгоритма.
Если задача гарантированно решается, но на решение уходит сто лет --- для человека она всё равно что неразрешима.
В этом смысле неразрешимость --- просто высшая степень недосягаемости решения.
\\ \\
Ну хорошо, идеальной формальной системы, в которой решались бы все задачи, нет и быть не может.
Но среди неидеальных, какая самая лучшая?
Какая позволяет решать наиболее широкий класс задач?
\\ \\
Вскоре после теоремы Гёделя на этот вопрос независимо ответили сразу несколько математиков.
Каждый из них предложил свою модель вычислений и на основе этой модели определил понятие \emph{вычислимости} функции:
Алан Тьюринг предложил машину Тьюринга, Алонсо Чёрч --- $\lambda$-исчисление, а Курт Гёдель --- рекурсивные функции.
Оказалось, что все предложенные модели, хотя и совершенно разные по форме, по сути совпадают: они описывают один и тот же класс функций.
Гипотеза Чёрча-Тьюринга предполагает, что этот класс совпадает с классом всех \emph{вычислимых на компьютере} функций.
\\ \\
Заметь: \emph{вычислимых на компьютере}.
Незаметно математики перешли от теории к практике.
Вычислимость на компьютере --- понятие глубоко физическое.
Оно привязывает абстрактную математику к вычислительным возможностям этого мира.
А на что способен наш мир --- никто не знает.
На что способен человеческий мозг, к примеру?
Что если он может решать более широкий класс задач, чем компьютер?
Даже кроме мозга, есть всякие неизведанные штуки: квантовая физика, чёрные дыры.
Наконец, ничто не мешает миру качественно измениться.
\\ \\
В этом смысле, гипотеза Чёрча-Тьюринга предполагает, что вычислительные возможности нашего мира
совпадают с вычислительными возможностями машины Тьюринга.
Что же это за штука --- машина Тьюринга?
Это очень простая абстрактная машина.
Вот её формальное определение: машина Тьюринга --- это семёрка $M = <Q, \Gamma, b, \Sigma, \delta, q_0, F>$, где
$$Q \ \text{--- конечное непустое множество состояний}$$
$$\Gamma \ \text{--- конечное непустое множество символов (алфавит)}$$
$$b \in \Gamma \ \text{--- пустой символ}$$
$$\Sigma \subseteq (\Gamma \textbackslash \{b\})\ \text{--- множество начальных символов}$$
$$q_0 \in Q \ \text{--- начальное состояние}$$
$$F \subseteq Q \ \text{--- множество конечных состояний}$$
$$\delta : (Q \textbackslash F) \times \Gamma \rightarrow Q \times \Gamma \times \{L, R\} \ \text{--- частично-определённая функция перехода}$$
Проще говоря, машина Тьюринга состоит из беконечной ленты-памяти, считывающей головки и функции переходов.
Головка считывает символ на ленте и в зависимости от текущего состояния записывает на ленту новый символ,
перемещается влево/вправо и переводит машину в новое состояние.
\\ \\
Если гипотеза Чёрча-Тьюринга --- правда, это значит, что люди никогда не построят более вычислительно-сильного компьютера.
Соответственно они не смогут решать задачи, неразрешимые на машине Тьюринга.
\\ \\
Если эта гипотеза --- неправда, то возможно людям удастся построить компьютеры, которые будут решать задачи,
неразрешимые на машине Тьюринга --- \emph{гиперкомпьютеры}.
Важно понимать, что есть большая разница между тем, чтобы \emph{придумать} гиперкомпьютер и \emph{построить} гиперкомпьютер.
Придумали их довольном много, в том числе сам Тьюринг: он предложил машину Тьюринга с оракулом
(и показал, что даже для машины с оракулом всё равно остаются неразрешимые задачи).
Другие гиперкомпьютеры основаны на таких же нереалистичных предположениях (в духе ``машина делает бесконечное число операций за конечное время'').
Были и более физически-обоснованные попытки (в том числе что-то там с чёрными дырами),
но в целом физический гиперкомпьютер вроде бы ещё никто не построил.
Одно могу сказать точно: если будут построены гиперкомпьютеры,
люди тут же зададутся вопросом, а нельзя ли построить гипергиперкомпьютеры. :D
\\ \\
Если вернуться к разрешимым на машине Тьюринга задачам, то даже они сильно различаются по сложности.
Теоретически для решения любой из них достаточно машины Тьюринга.
Но это всё равно что микроскопом клопов давить: можно, но не самое простое и эффективное средство.
Поэтому задачи разбиваются на классы сложности: все задачи из одного класса имеют примерно одинаковую сложность.
В каждом классе выбирается ``самая сложная'' задача, к которой сводятся все остальные (то есть являются её частным случаем).
Выбор задачи-представителя --- простой и очень сильный приём.
Он позволяет делать выводы о целом классе задач на основании одной-единственной задачи-представителя:
например, можно доказать равенство двух классов, доказав сводимость их представителей друг к другу,
или выяснить алгоритмическую сложность неизвестной задачи, сведя её к какой-нибудь известной.
\\ \\
Для каждого класса задач люди придумали модель вычислений, позволяющую эффективно решать задачи из этого класса.
Я не буду приводить иерархию этих классов (и соответствующих моделей вычислений):
во-первых, я в этой теме слаба, а во-вторых --- существует множество подклассов со своими моделями вычислений,
и эти подклассы находятся в странных (не всегда даже известных) взаимоотношениях.
Некоторые примеры моделей вычислений я упоминала, когда говорила про формальные грамматики.
\\ \\
В математике для описания сложности алгоритма принята нотация ``О большое'' (``Big O'' notation):
$$f(x) = O(g(x))$$
$$\Leftrightarrow$$
$$\exists M>0, x_0 \in \mathbb{R} \ \ \text{такие, что}\ \  \forall x>x_0: |f(x)| \leq M |g(x)|$$
Функция $f(x)$ может быть функцией времени, памяти или чего угодно (обычно времени).
\\ \\
Но вернёмся к баранам, то есть языкам программирования.
Тьюринг-полнота --- то есть способность решать все задачи, которые решает машина Тьюринга ---
важное свойство для языков общего назначения.
Специализированные языки могут не быть Тьюринг-полными --- они предназначены для узкого круга задач
(например, язык работы с базами данных SQL не Тьюринг-полный, регулярные выражения не Тьюринг-полны).
\\ \\
Выразительную силу языка можно понимать и в другом, практическом смысле:
насколько хорошо язык позволяет работать с разными штуками в реальной жизни (файловая система, сетевые взаимодействия и т.д).
Поскольку программы обычно исполняются в операционной системе, язык должен предоставлять удобные средства работы
с операционной системой.

\section{Структура языка}
Итак, язык программирования --- это синтаксис и семантика.
Программист, однако, смотрит на язык скорее как на набор инструментов, каждый из которых нужен для выражения какой-то конкретной идеи.
Разные языки предоставляют разный набор инструментов.
Я перечислю некоторые важные составляющие языка:
\begin{itemize}
\item переменные
\item управление памятью
\item cистема типов
\item поток управления
\item абстракция
\item области видимости
\item ввод/вывод
\item доступ к кишкам
\end{itemize}
И немного поговорю про каждую из них.

\subsubsection{Переменные}
Переменная --- это способ обозначить какую-то изменяющуюся величину.
В императивных языках программирования переменная --- это обозначение куска памяти (l-value).
В функциональных языках переменная не привязана к памяти, это просто обозначение выражения (binding).
В некоторых языках переменные не могут изменяться (immutable), то есть они скорее ``постоянные''.
Но в целом идея та же: переменная --- это обозначение.

\subsubsection{Управление памятью}
Данные, с которыми программа работает (и саму программу) надо хранить в памяти.
Различают \emph{статическую} и \emph{динамическую} память.
\\ \\
В статической памяти хранятся те данные, размер которых известен при старте программы и не изменяется во время исполнения.
Сами данные при этом могут меняться, но остаются в той же памяти.
Статическая память выделяется один раз, при запуске программы, и освобождается при завершении.
\\ \\
В динамической памяти хранятся данные, которые возникают и исчезают по ходу исполнения прогрммы.
Количество и размер этих данных не известны заранее, они зависят от входных данных (и, возможно, фазы луны).
\\ \\
Одни языки отдают управление динамической памятью на откуп программисту: он сам должен следить за выделением новой памяти и освобождением старой.
При таком подходе программисты делают много ошибок: забывают освободить память,
или освобождают, а потом обращаются к уже затёртым данным.
Языки с ручным управлением памятью жертвуют безопасностью ради эффективности.
Программы на них могут быть быстрыми, но их надо дополнительно проверять на отсутствие ошибок памяти.
Примеры языков с ручным управлением памятью: Assembler, Algol, C, C++, Cobol, Forth, Fortran, Pascal.
\\ \\
Другие языки осуществляют автоматическое управления памятью --- \emph{сборку мусора} (garbage collection, GC).
Программист в таких языках почти не имеет доступа к упавлению памятью.
Сборка мусора --- это безопасность, купленная ценой чудовищной неэффективности:
сборщик мусора должен во время исполнения программы периодически сканировать всю память, искать ненужные куски и освобождать их.
Причём, в отличие от программиста, сборщик мусора мало что знает о времени жизни объектов в памяти:
ему приходится отслеживать все ссылки на объект и удалять его только когда не осталось ссылок.
Это очень тормозит программу: нередки случаи, когда GC съедает 90\% времени.
Примеры языков со сборщиком мусора: Lisp, Java, Javascript, Haskell, Perl, Prolog, Ruby.
\\ \\
Наконец, в последнее время появляются языки, которые пытаются отказаться от сборки мусора,
но в то же время не дать программисту наделать ошибок в управлении памятью.
Сборщик мусора в таком языке может присутствовать, но быть необязательным, нежелательным, выключенным по умолчанию.
Это, конечно, самый лучший вариант: он сочетает эффективность и безопасность.
Я пока знаю один пример --- Rust, но язык --- экспериментальный и у него много проблем (прим. Сергея Анатольевича).

\subsubsection{Система типов}
Тип --- это множество значений.
Изначально типы выдумали математики, чтобы избавиться от противоречий вроде парадокса Рассела:
``Пусть K — множество всех множеств, которые не содержат себя в качестве своего элемента. Содержит ли K само себя в качестве элемента?''.
Если потребовать, чтобы множество состояло из элементов одного типа, то это множество не может содержать само себя,
а вопрос некорректен.
Типы спасли математиков от парадоксов, но оказалось, что они накладывают на математику ограничения,
не позволяющие выразить некоторые простые и важные вещи.
Пришлось добавлять некую сомнительную ``аксиому сводимости'' (axiom of reducibility) и прочие костыли.
\\ \\
В языке программирования типы --- одна из светлых и прекрасных идей, они вносят в прорамму ясность.
Типы могут быть \emph{простыми} (число, строка, etc.),
а могут быть \emph{составными}, то есть построенными из других типов (структуры, классы, записи, перечисления ---
всё это примеры составных типов данных в разных языках).
Отношения между типами данных могут быть самыми разными: непересекающиеся множества, подмножества и т.д.
В некоторых языках можно выстраивать сложные иерархии типов.
\\ \\
У системы типов есть следующие ортогональные характеристики:
\begin{itemize}
\item \textbf{статическая/динамическая}
    \\
    Если типизация статическая, все типы проверяются на этапе компиляции, во время исполнения программы типов уже нет.
    Если типизация динамическая, типы проверяются во время исполнения программы.
    Динамическая типизация имеет два чудовищных недостатка: во-первых --- программа становится намного медленнее
    (из-за проверок типов в рантайме и невозможности целого ряда оптимизаций),
    во-торых --- все ошибки типов выясняются только во время исполнения.
    Многие люди любят динамическую типизацию за её ``естественность'', но лично я её злостно недолюбливаю за медленность.
    Примеры: статическая: C, Haskell, Java, C\#; динамическая: Python, JavaScript, Ruby.

\item \textbf{явная/неявная}
    \\
    Иногда программисту не надо указывать тип --- языковой процессор может вывести тип самоcтоятельно, на основании уже известных типов в программе.
    Если информации недостаточно, чтобы вывести тип, языковой процессор может выдать ошибку или выбрать дефолтный вариант.
    Неявная типизация --- это когда типы можно не указывать (или даже нельзя указывать), явная --- когда обязательно указывать.
    В общем, это дело вкуса: я предпочитаю видеть типы, но иногда они занимают очень много места.
    Примеры: явная --- C++, D, C\#; неявная: PHP, Lua, JavaScript.

\item \textbf{строгая/нестрогая (сильная/слабая)}
    \\
    Языки со строгой типизацией не позволяют смешивать разные типы.
    Обычно есть возможность \emph{привести} один тип к другому, но такое приведение должно быть явным.
    Языки с нестрогой типизацией автоматически (неявно) выполняют приведение типов по мере надобности
    Иногда это бывает удобно (например, складывать целые числа с действительными), но в большинстве случаев приводит к ошибкам.
    В сущности, пропадает главное достоинство типизации: защита от дурака (то есть самого программиста).
    Примеры: строгая --- Java, Python, Haskell, Lisp; нестрогая: C, JavaScript, Visual Basic, PHP.
\end{itemize}
Очень важно не смешивать эти понятия: не думать, что динамическая типизация обязательно нестрогая,
или что неявная типизация --- это когда есть неявное приведение типов.
Люди постоянно путают и мешают всё в кучу, да это и понятно: они судят о типизации по какому-нибудь знакомому языку.
Например, типизация Javascript'а --- динамическая, неявная и нестрогая,
поэтому для джаваскриптовиков понятие динамичности прочно увязывается с понятием нестрогости и даже какой-то расхлябанности:
делай что хочешь, интерпретатор съест.
В Haskell'е типизация статическая, наполовину неявная и строгая --- поэтому у хаскеллистов статичность увязывается со строгостью
(если программа на хаскеле компилируется, значит в ней отсутствует целый класс потенциальных ошибок).
\\ \\
Бывают и языки совсем без типов (например, Assembler).

\subsubsection{Поток управления}
Программа на любом языке подразумевает выполнение \emph{действий} для достижения \emph{результата}.
Форма, в которой записана программа, может быть очень разной, но два основных подхода такие:
\begin{itemize}
\item \textbf{императивный}: программа --- это список инструкций исполнителю
\item \textbf{декларативный}: программа --- это описание результата, который нужно получить
\end{itemize}
Обычно эту форму называют \emph{парадигмой программирования} и выделяют ещё много других парадигм:
объектно-ориенированную, функциональную, логическую и т.д.
Однако около всех этих парадигм возникла неслабая путаница, в результате которой мало кто понимает,
что означает, к примеру, функциональное программирование: то ли это языки, в которых есть функции высших порядков
(high-order functions, HOF), то ли языки в которых функции являются чистыми функциями своих аргументов (нет side-effect'ов),
то ли просто языки, которые хорошо выполняют свою функцию. :D
Кроме того, ничто не мешает объектно-ориентированному языку быть при этом функциональным.
Лучше вообще не употреблять слова ``парадигма'', потому что непонятно, к чему оно относится.
Вот ``поток управления'' --- это понятно: это порядок выполнения действий для достижения результата.
\\ \\
Ты наверное заметила, что выделенные мной формы задания потока управления
(императивный/декларативный) сильно смахивают на виды семантики (операционная/денотационная).
Как и с семантикой, подчёркиваю, упреждаю, предостерегаю, наконец, попросту говорю:
это мне внутреннее чувство не даёт выделить другие виды как что-то принципиально новое,
а в миру бытуют разные мнения. :D
\\ \\
В императивных языках порядок, в котором записана программа, отражает порядок выполнения действий.
Если два действия записаны последовательно друг за другом, они так же последовательно будут выпоплнены.
Есть разные способы передачи управления: безусловные переходы, условные переходы, циклы, процедуры и функции.
Программист управляет тем, \emph{как} вычисляется результат.
Примеры императивных языков: Assembler, C, C++, Java, Pascal.
\\ \\
В декларативных языках порядок, в котором записана программа, не важен:
программа --- это описание конечного результата, и порядок действий определяется зависимостью
промежуточных результатов друг от друга.
Если конечный результат зависит от промежуточного --- промежуточный должен быть вычислен раньше.
Программист не управляет напрямую тем, \emph{как} вычисляется результат --- он только говорит, \emph{что} надо получить.
Поэтому в декларативных языках нет способов передачи управления, вместо них есть способы описания зависимостей.
Примеры декларативных языков: SQL, HTML, Prolog, регулярные выражения, с большего Haskell.
\\ \\
Есть ещё одна важная штука: параллелизм.
Часто какие-то независимые действия в программе можно выполнять параллельно.
Если процессор многоядерный, речь идёт о настоящем параллелизме: одновременно выполняются несколько потоков управления.
Настоящий параллелизм может давать неслабый прирост производительности (почитай про закон Амдала).
Есть ещё псевдопараллелизм, когда потоки управления выполняются на одном и том же процессоре,
просто по очереди: немного повыполнялся один, потом следующий и т.д.
Обычно потоков управления больше, чем ядер процессора (особенно если одновременно выполняется много программ),
поэтому псевдопараллелизм надстраивается поверх настоящего параллелизма:
все потоки управления выстраиваются в очередь, а из очереди распределяются на разные ядра.
\\ \\
Одни языки лучше приспособлены для параллелизма (легко отслеживать зависимости между данными),
другие --- хуже (много запутанных зависимостей).
Между тем роль параллелизма растёт: в последнее время ускорять процессоры стало почти некуда,
и мощности стали наращивать вширь: делать многоядерные процессоры и многопроцессорные архитектуры.
Поэтому для языка программирования крайне важно предоставлять простые и эффективные средства распараллелить программу.

\subsubsection{Абстракция}
Иногда есть много похожих вещей, объединённых какой-то общей чертой.
Это могут быть похожие структуры данных, похожие функции, похожие типы или что угодно ещё.
Если эта похожесть --- не случайное, а неотъемлемое качество, её обязательно надо зафиксировать
(как инвариант, который не должен нарушаться).
Если одна и та же идея реализована несколько раз, приходится постоянно следить, чтобы все реализации делали одно и тоже,
и почти неизбежно вкрадутся различия и ошибки.
Наконец, просто написать много одинакового кода --- уже неприятно.
Поэтому у языка должны быть средства выразить похожесть,
то есть абстрагироваться от мелких отличий и выразить общую идею --- средства абстракции.
Вот некоторые из них:
\begin{itemize}
\item \textbf{структурное программирование}
    \\
    Это разбиение программы на подпрограммы: модули, процедуры, функции и т.д.
\item \textbf{параметирический полиморфизм}
    \\
    Параметрический полиморфизм --- это когда есть одна общая для всех возможных параметров реализация идеи.
    Эта реализация не делает никаких предположений о передаваемом в неё параметре, она максимально общая и универсальная.
    Это как бы аналитически заданная всюду определённая функция параметра.
\item \textbf{ad-hoc полиморфизм}
    \\
    Ad-hoc, или ``абы-какой'' полифорфизм --- это когда идея имеет несколько разнородных реализаций для отдельных параметров.
    Каждая реализация особенная, подходит только для этого конкретного параметра.
    Параметры, для которых идея не реализована --- в пролёте.
    Это как бы таблично заданная частично определённая функция параметра.
\item \textbf{полиморфизм подтипов}
    \\
    Полиморфизм подтипов, или полиморфизм включения (subtyping) --- это когда есть иерархия, основанная на включении:
    потомки наследуют свойства предков.
    При этом потомки могут менять унаследованные свойства.
    Получается, что идея реализована для всей иерархии (у одних потомков переопределённая реализация, у других --- унаследованная),
    но вне иерархии не существует.
\item \textbf{DSL}
    \\
    DSL --- Domain Specific Language.
    Некоторые языки позволяют легко писать на них маленькие специализированные языки --- DSL'и.
    Такой DSL затачивается под определённый круг задач, и эти задачи решаются на нём в несколько строчек.
    Конечно, любой DSL можно написать с нуля хотя бы и на ассемблере, то есть он ничем принципиальнвм не отличается от обычного языка.
    Но с реализацией обычного языка долго мучаются, а DSL'и пишут быстро и легко за счёт средств исходного языка.
\end{itemize}
Абстракция может относиться к потоку управления (структурное программирование), данным (полифморфизмы)
или целому языку (DSL'и).
Вообще, язык высокого уровня --- сам по себе абстракция над языками низкого уровня.
\\ \\
Хорошие абстракции --- это такие, которые не ухудшают эффективность программы,
то есть программист не стоит перед мучительным выбором: стройная, но медленная программа или распухшая, но быстрая.
Про такие абстракции говорят ещё ``нулевая абстракция''.
Но довольно часто абстракция вынуждает жертвовать эффективностью:
например, один общий сложный алгоритм для решения многих задач (некоторые из которых можно решить проще).
Особенно хорошо это чувствуется в достаточно низкоуровневых языках (типа C/C++):
часто какая-то библиотечная функция слишком медленная, потому что рассчитана на общий случай и делает много лишнего.

\subsubsection{Области видимости}
Область видимости (scope) --- это часть программы, в пределах которой идентификатор продолжает быть связанным со своим значением.
Есть два главных вида областей видимости:
\begin{itemize}
\item \textbf{лексическая (статическая)}, или \emph{раннее связывание} ---
    под ``областью программы'' понимается часть исходного кода, в которой объявлен идентификатор.
\item \textbf{динамическая}, или \emph{позднее связывание} ---
    под ``областью программы'' понимается состояние программы во время исполнения, когда встретился идентификатор.
\end{itemize}
Приведу для понятности хороший пример с википедии. Это программа на bash'е:
\begin{verbatim}
    x=1
    function g () { echo $x ; x=2 ; }
    function f () { local x=3 ; g ; }
    f
    echo $x
\end{verbatim}
У bash'а динамическая область видимости, поэтому программа выведет 3 и 1.
Точно такая же программа на С:
\begin{verbatim}
    #include <stdio.h>

    int x = 1;

    void g () {
        printf ("%d", x);
        x = 2;
    }

    void f () {
        int x = 3;
        g ();
    }

    int main () {
        f ();
        printf ("%d", x);
        return 0;
    }
\end{verbatim}
выведет 1 и 2, потому что в C лексическая область видимости.
\\ \\
Области видимости можно классифицировать по их ``ширине охвата'':
\begin{itemize}
\item глобальная
\item модуль
\item единица трансляции
\item функция
\item блок
\item выражение
\end{itemize}

\subsubsection{Ввод/вывод}
Полезный язык предоставляет возможности для взаимодействия с внешним миром:
файловой системой, сетью, другими программами, устройствами ввода/вывода и прочим.
Обычно это просто обёртки над системными вызовами, но, поскольку хороший язык должен быть портабельным,
он должен учитывать разные операционные системы.
Например, хорошо бы, чтобы одна и та же программа читала файлы и на линуксе, и на винде.
\\ \\
Такая портабельность даётся непросто (особенно если все разработчики языка живут на какой-то одной системе),
но она очень важна: это одно из свойств, за которые любят высокоуровневые языки.
Никому не хочется мучиться с разными архитектурами, хочется получить портабельность за бесплатно.

\subsubsection{Доступ к кишкам}
Некоторые языки позволяют прямо во время исполнения программы
узнавать что-то о программе (introspection),
менять саму программу (reflection, self-modifying code)
или просто динамически генерировать код (dynamic code generation).
\\ \\
Например, C++ позволяет узнать тип объекта во время исполнения (\texttt{typeid} и \texttt{dynamic\_cast}).
\\ \\
А в Javascript'е есть функция \texttt{eval}, принимающая строчку, которая интерпретируется как код программы.
Функция \texttt{eval} прямо на ходу парсит и исполняет эту строчку.
Например, такая программа:
\begin{verbatim}
    eval("2 + 3")
\end{verbatim}
выведет 5. А вот такой пример:
\begin{verbatim}
    a = 1
    s = "print(a++); if (a <= 10) eval(s)"
    eval(s)
\end{verbatim}
выведет 1, 2, 3, 4, 5, 6, 7, 8, 9 и 10.
Можно на ходу создавать функции или менять уже существующие:
\begin{verbatim}
    eval = function () { print("no eval!") }
    eval("2 + 3")
\end{verbatim}
выведет, уже не 5, а ``no eval!''.
\\ \\
Некоторые языки позволяют динамически генерировать и исполнять не код, а какое-нибудь промежуточное представление: байткод или AST.
В примере из последней главы мы будем строить маленький JIT-компилятор
и тоже займёмся динамической генерацией кода (машинного).
\\ \\
Всё это интересные возможности, но довольно часто они приводят к нехорошим последствием:
программист начинает писать запутанные программы, по которым непонятно, что они делают, пока их не начнёшь выполнять.
Во время исполнения начинаются какие-то ковыряния в кишках интерпретатора и прочие странные махинации.
Обычно это говорит либо о том, что программист не осилил что-то сделать нормальными средствами,
либо что он намеренно хочет запутать программу (недаром на Javascript'е пишут столько вирусов и троянов).
Но есть и другие примеры использования: например, ранние компиляторы C были ограничены таким маленьким объёмом памяти
(размер самой программы был ограничен), что им приходилось на ходу перетирать свой же код более новым.
Да и упомянутые JIT-компиляторы должны на ходу генерировать и исполнять код.

\section{Итоги}
Язык программирования состоит из синтаксиса и семантики.
\\ \\
Синтаксис языка может быть определён с помощью формальных грамматик.
Иерархия Хомского --- это классификация формальных грамматик по правилам вывода:
неорганиченные, контекстно-зависимые, контекстно-свободные, регулярные.
Для языков программирования наиболее важны два класса грамматик: регулярные (они используются для описания лексем)
и контекстно-свободные (они используются для описания связей между лексемами).
\\ \\
Семантика языка определяет его выразительную силу, то есть круг задач, которые можно решать на этом языке.
На сегодняшний день выразительная сила всех языков ограничена выразительной силой машины Тьюринга.
\\ \\
Синтаксис и семантика до определённой степени взаимозаменяемы:
сематические особенности можно выразить с помощью более сложного синтаксиса,
а синтаксически трудновыразимые тонкости можно взвалить на семантику.
\\ \\
В языке программирования очень важны: выразительность, эффективность, защита от дурака, взаимодействие с внешним миром.



\chapter{Машинный язык}
Компьютер --- это программируемое вычислительное устройство.
Он состоит из четырёх основных компонент:
\begin{itemize}
\item процессор
\item память
\item устройство ввода
\item устройство вывода
\end{itemize}
В этой главе я приведу грязно перевранную выжимку из книги Чарльза Петцзольда ``Code'':
попытаюсь кратко объяснить, что такое машинный язык.

\section{Электричество и логика}
Компьютеры придумали для автоматизации вычислений.
Первый компьютер, придуманный Чарльзом Бэббиджем, был механическим и основанным на десятичной системе счисления.
Из-за своей сложности он так и не был построен.
Понадобилось сделать два серьёзных упрощения, чтобы построение компьютеров стало возможным:
от механики перейти к электричеству, а от десятичной системы счисления --- к двоичной.
\\ \\
В принципе, компьютеры не обязательно строить на основе электрических схем.
Более того, не исключено, что есть какие-то другие физические процессы,
которые позволят построить более вычислительно-мощные компьютеры.
Но чтобы привязаться хоть к какой-то реальности, я рассмотрю несколько простых электрических схем,
на основе которых можно собрать схемы любой сложности.
\\ \\
Начнём с простой схемы:
\\ \\
\includegraphics[height=0.7in]{pic/2.png}
\\ \\
В этой цепи не много элементов: лампочка, батарея, ключ. Если ключ разомкнут,
ток не идёт, и лампочка не горит. Если ключ замкнут, ток идёт, и лампочка горит.
(Кстати, лампочка нас интересует только как индикатор, есть ток или нет.)
Добавим ещё один ключ в схему (последовательно):
\\ \\
\includegraphics[height=0.7in]{pic/4.png}
\\ \\
Теперь лампочка зависит от двух ключей: она горит, только если первый \emph{и} второй ключ замкнут.
Если бы мы добавили второй ключ параллельно, схема бы получилась такой:
\\ \\
\includegraphics[height=1in]{pic/6.png}
\\ \\
В этой схеме лампочка горит, только если первый \emph{или} второй ключ замкнут.
\\ \\
Мы получили две интересных схемы: первая отражает логическое И (AND), вторая ---
логическое ИЛИ (OR). На основе этих схем можно собирать другие, более сложные.
Например, есть у тебя условие: ``если я сдам лабу или будет солнечно,
то я пойду гулять''. Это условие описывается схемой ИЛИ:
\\ \\
\includegraphics[width=2in]{pic/8.png}
\\ \\
Преподаватель, которому ты собралась сдавать лабу, знает, что если у него
будет хорошее настроение, то ты сдашь лабу. Его настроение зависит от двух вещей:
во-первых, будет ли солнечно, а во вторых, успеет ли он пообедать.
Он может описать это с помощью схемы И:
\\ \\
\includegraphics[width=2in]{pic/9.png}
\\ \\
Допустим, преподаватель хочет узнать, пойдёшь ли ты завтра гулять, но не очень
хорошо разбирается в хитросплетениях твоей логики. Зато у него есть твоя схема.
Он берёт её и пытается присобачить к ней свою. Логически это очень просто:
нужно связать выход ``хорошее настроение'' со входом ``сдам лабу'': если на выходе
``хорошее настроение'' есть ток, ключ ``сдам лабу'' замкнут, если тока нет --
разомкнут:
\\ \\
\includegraphics[width=3in]{pic/10.png}
\\ \\
Нужно что-то, что автоматически замыкало и размыкало бы ключ в зависимости от тока.
Одним из таких устройств является реле:
\\ \\
\includegraphics[height=1in]{pic/11.png}
\\ \\
Если в первой цепи течёт ток, то катушка намагничивается и начинает притягивать
ключ во второй цепи. Ключ замыкается. Если в первой цепи ток пропадает, то катушка размагничивается,
перестаёт притягивать ключ и он размыкается. Реле позволяет связать вход
``сдам лабу'' с выходом ``хорошее настроение'':
\\ \\
\includegraphics[width=3in]{pic/12.png}
\\ \\
Конечно, эту конкретную схему можно упростить и выкинуть реле, просто встроив преподавательскую схему
в твою вместо ключа ``сдам лабу'':
\\ \\
\includegraphics[width=2in]{pic/13.png}
\\ \\
И даже ещё сильнее упростить:
\\ \\
\includegraphics[width=2in]{pic/14.png}
\\ \\
Но в общем случае реле позволяет сделать очень важное предположение:
что выход любой схемы совместим со входом любой другой схемы,
то есть можно собирать из простых схем более сложные.
(В телеграфах реле использовалось для усиления слабого тока при передаче сигнала на большие растояния.)
\\ \\
Реле --- не единственное устройство, позволяющее управлять током с помощью тока.
Сейчас используются транзисторы, а до этого были вакуумные лампы.
\\ \\
Вот как выглядят схемы И и ИЛИ с использованием реле:
\\ \\
\includegraphics[height=1in]{pic/15.png}
\\ \\
Строго говоря, это уже не схемы, а фрагменты схем. Но, с другой стороны, ``выход'' схемы ---
тоже не выход, а кусок цепи, по которому идёт или не идёт ток. Чтобы соединить выход
и фрагмент, нужно разомкнуть цепь в районе выхода и получившиеся два конца провода примотать
к концам провода, торчащим из фрагмента:
\\ \\
\includegraphics[height=1.5in]{pic/16.png}
\\ \\
Дальше я всё буду называть схемой. Вот ещё одна важная схема, отражающая логическое НЕ (NOT):
\\ \\
\includegraphics[height=1in]{pic/17.png}
\\ \\
В предыдущих схемах реле повторяло сигнал, а здесь оно его инвертирует: если в первой цепи идёт ток, то во второй тока нет, и наоборот.
Поэтому эта схема называется \emph{инвертором}.
\\ \\
Логические функции И, ИЛИ, НЕ образуют \emph{полную систему булевых функций} --- через них можно выразить любую логическую функцию.
(Вообще, это не минимальная полная система: с помощью законов де Моргана ИЛИ выражается через И и НЕ, а И --- через ИЛИ и НЕ,
но мне удобнее иметь под рукой три схемы.)
\\ \\
Для этих трёх схем --- И, ИЛИ, НЕ --- я введу (общепринятые) условные обозначения:
\\ \\
\includegraphics[height=2in]{pic/logic_legend.png}

\section{Сумматор}
Вот устройство, способное сложить два бита --- полусумматор (half-adder):
\\ \\
\includegraphics[height=1.5in]{pic/half_adder.png}
\\ \\
У полусумматора два входа (для двух однобитных слагаемых)
и два выхода (для однобитной суммы и переноса на следующий разряд, поскольку сумма может быть двухбитной).
\\ \\
Чтобы складывать многобитные числа, в каждом разряде кроме двух слагаемых нужно учитывать ещё и перенос с предыдущего разряда.
Вот схема, способная сложить три бита --- сумматор (adder):
\\ \\
\includegraphics[height=1.5in]{pic/adder.png}
\\ \\
У сумматора три входа (для двух однобитных слагаемых и переноса с предыдущего разряда)
и два выхода (для однобитной суммы и переноса на следующий разряд).
\\ \\
Соединяя сумматоры в цепочку, мы можем собрать сумматор для N-битных чисел.
Вот, например, 8-битный сумматор:
\\ \\
\includegraphics[height=2.5in]{pic/n_bit_adder.png}
\\ \\
У 8-битного сумматора два 8-битных входа $A_0 - A_7$ и $B_0 - B_7$ (для двух 8-битных слагаемых)
и один 9-битный выход $C_0 - C_8$ (сумма двух 8-битных чисел может быть 9-битным числом).

\section{Команды}
Примерно так же, как N-битный сумматор, можно собрать N-битный вычитататель, умножатель, делитель и т.д.
Допустим, мы собрали много разных N-битных схем и хотим объединить их в один большой N-битный калькулятор.
У калькулятора должно быть два N-битных входа для данных и ещё какой-то вход для выбора нужного действия.
\\ \\
Для выбора действия нам понадобятся два устройства.
\\ \\
Первое устройство --- дешифратор --- отвечает за подачу входных данных.
Оно должно перенапрвалять операнды на вход выбранной схемы, а на вход остальных схем подавать ноль.
Вот пример дешифратора 2 на 4:
\\ \\
\includegraphics[width=3in]{pic/decoder_2_to_4.png}
\\ \\
Входы $X_0$ и $X_1$ --- управляющие.
Меняя комбинацию битов на этих входах, можно управлять тем, на какой из выходов $B_0$, $B_1$, $B_2$ или $B_3$ попадёт бит на входе $A_0$:
значение пары $(X_0, X_1)$, равное $(0, 0)$ кодирует выход $B_0$, $(0, 1)$ --- $B_1$, $(1, 0)$ --- $B_2$ и $(1, 1)$ --- $B_3$.
На остальных выходах сигнал равен нулю.
\\ \\
Второе устройство --- селектор --- отвечает за считывание результата.
Оно должно считывать результат с выхода выбранной схемы, а выходы остальных схем игнороровать.
Вот пример селектора 4 на 1:
\\ \\
\includegraphics[width=3in]{pic/selector_4_to_1.png}
\\ \\
Входы $X_0$ и $X_1$ --- управляющие.
Меняя комбинацию битов на этих входах, можно управлять тем, какой из входов $A_0$, $A_1$, $A_2$ или $A_3$ попадёт на выход $B_0$:
значение пары $(X_0, X_1)$, равное $(0, 0)$ кодирует вход $A_0$, $(0, 1)$ --- $A_1$, $(1, 0)$ --- $A_2$ и $(1, 1)$ --- $A_3$.
Сигнал на остальных выходах игнорируется.
\\ \\
Чтобы применить это к N-битным схемам, нужно на каждый бит входа повесить по дешифратору, а на каждый выход --- по селектору.
Например, для четырёх схем с двумя 8-битными входами и 9-битным выходом (например, сложение, вычитание, умножение и деление) схема была бы такой:
\\ \\
\includegraphics[height=9in]{pic/commands.png}
\\ \\
В зависимости от комбинации бит на входе селектор активирует одну из четырёх схем: 11 --- сложение, 10 --- вычитание, 01 --- умножение, 00 --- деление.
Схема, конечно, не без недостатков: результат любой операции 9-битный, как у сложения, хотя результат вычитания и деления --- 8-битный, а умножения --- 16-битный.
Зато схема на всю страницу. :D
\\ \\
Получился этакий простецкий калькулятор.
Мы управляем калькулятором, подавая на вход какую-то комбинацию бит.
Эта комбинация бит --- это \emph{команда} на \emph{машинном языке}.
\\ \\
В общем-то, на этом можно было бы остановиться: уже должно быть понятно, что такое машинный язык.
Но на пути от калькулятора к компьютеру осталось ещё два серьёзных прорыва.

\section{Память}
Что можно сделать с результатом вычислений нашего калькулятора?
Можно ли его использовать для дальнейших вычислений в самом калькуляторе?
Результат представляет собой набор электрических сигналов на выходе схемы,
который напрямую зависит от сигналов на входе: поменяешь вход --- тут же поменяется выход.
Поэтому, если мы попытаемся подать результат на вход схемы для дальшейшего использования,
ничего не получится: схема тут же среагирует и поменяет результат на выходе, что повлечёт изменение входа и т.д.
Нужен какой-то способ отвязать результат от входа калькулятора, сохранить его независимо от изменений на входе.
\\ \\
Нам нужна схема, обладающая способностью \emph{запоминать} сигнал.
В этом нам поможет однобитная защёлка:
\\ \\
\includegraphics[height=1.5in]{pic/latch.png}
\\ \\
Эта схема принципиально сложнее предыдущих, потому что в ней используется обратная связь.
У схемы два входа: вход $D$ для однобитовых данных (data) и вход $W$ для управления записью (write).
Когда вход $W=1$, выход $Q$ копирует значение на входе $D$.
Когда вход $W=0$, никакие изменения на входе $D$ не влияют на выход $Q$ --- он сохраняет последнее значение входа $D$ при $W=1$.
\\ \\
Схема, лежащая в основе защёлки (из двух ИЛИ и двух НЕ с обратной связью) называется RS-триггером (Reset - Set).
Именно RS-триггер реализует запомининие, а остальные элементы схемы (два И и одно НЕ) реализуют удобный переключатель для записи/хранения бита.
\\ \\
Из однобитных защёлок очень просто собрать многобитную: надо просто объединить их входы $W$.
Например, 8-битная защёлка выглядит так:
\\ \\
\includegraphics[height=1.5in]{pic/latch_8_bit.png}
\\ \\
С помощью таких защёлок можно сохранять результат вычислений калькулятора, а потом, когда будет надо,
подавать его на вход для дальнейшего использования.
\\ \\
\begin{minipage}{0.7\textwidth}
Более того, можно собрать сразу много N-битных защёлок и выбирать, в какую из них записывать или считывать данные
(с помощью дешифратора и селектора, как и с выбором команд).
Это уже получится память с произвольным доступом (Random Access Memory, RAM).
\\ \\
С помощью M-битного дешифратора и селектора можно адресовать $2^M$ N-битных ячеек памяти.
\end{minipage}
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=0.7\textwidth]{pic/ram.png}
\end{minipage}
\\
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[width=0.7\textwidth]{pic/calculator_ram.png}
\end{minipage}
\begin{minipage}{0.7\textwidth}
Изначально мы собирали память для того, чтобы хранить в ней промежуточные результаты вычислений на калькуляторе.
Но ведь в памяти можно хранить не только данные, но и команды калькулятора ---
они, как и данные, кодируются битовыми последовательностями.
Вместо того, чтобы задавать команды вручную, можно считывать их из памяти и подавать на вход калькулятора.
\end{minipage}

\section{Автоматизация}
Итак, у нас есть программируемый калькулятор.
Не хватает одной мелочи: оживить его, чтобы он как-то сам по себе читал команды из памяти и исполнял их.
Для этого нам понадобится счётчик.
Счётчик состоит из двух частей: осциллятора и делителя частоты.
\\ \\
Осциллятор прост:
\\ \\
\includegraphics[height=0.5in]{pic/oscillator.png}
\\ \\
Выход осциллятора колкблется между 0 и 1:
\\ \\
\includegraphics[height=1in]{pic/oscillator_plot.png}
\\ \\
Делитель частоты не так прост.
Он построен на основе D-триггера.
D-триггер похож на RS-триггер.
У него тоже два входа: $D$ --- для данных, и $W$ --- для управления записью в триггер.
Ключевое отличие D-триггера от RS-триггера заключается в условии, при котором происходит запись (то есть сигнал на входе $D$ копируется на выход триггера):
если в RS-триггере это условие $W=1$, то в D-триггере это условие $W=\uparrow$, т.е. изменение сигнала на входе $W$ с 0 на 1.
D-триггер состоит из двух RS-триггеров:
\\ \\
\includegraphics[height=2in]{pic/d_trigger.png}
\\ \\
Если соединить выход $\overline{Q}$ D-триггера со входом $D$, то получится делитель частоты ---
схема, которая вдвое уменьшает частоту входного сигнала.
Например, если ко входу $W$ присоединить осциллятор, выходной сигнал которого колеблется с частотой $t$,
то на выходе $Q$ мы получим сигнал, колеблющийся с частотой $t/2$:
\\ \\
\includegraphics[height=1.5in]{pic/frequency_divider.png}
\\ \\
Если последовательно соединить осциллятор и несколько делителей частоты, то получится счётчик:
\\ \\
\includegraphics[height=2in]{pic/counter.png}
\\ \\
Выходы $A_0$, $A_1$, $A_2$, $A_3$ образуют двоичное число, которое увеличивается на единицу кажую единицу времени
(если $A_0=1$, $A_1=1$, $A_2=1$, $A_3=1$, то в следующий момент времени счётчик обнуляется).
\\ \\
\begin{minipage}{0.6\textwidth}
Если присоединить выход счётчика ко входу селектора памяти,
то значение счётчика будет интерпретироваться как адрес в памяти.
Получится, что счётчик последовательно перебирает все ячейки памяти.
Если при этом выход селектора памяти соединить со входом для команд калькулятора,
а в памяти последовательно хранить команды,
то счётчик будет раз в единицу времени подавать новую команду на вход калькулятора.
\end{minipage}
\begin{minipage}{0.4\textwidth}
\includegraphics[width=\textwidth]{pic/automation.png}
\end{minipage}
\\ \\
Примерно так можно автоматизировать выборку команд из памяти.
Остаётся вопрос, где хранить данные и как синхронизировать подачу команд и данных на вход калькулятора.
\\ \\
Самый простой, негибкий вариант --- хранить данные в отдельном массиве, в порядке,
строго соответствующем порядку команд: чтобы по сигналу счётчика
одновременно выбиралась команда и данные для неё.
\\ \\
Более гибкий вариант --- кодировать адрес данных в самой команде:
тогда сначала придётся загрузить из памяти команду, а схема, отвечающая за исполнение этой команды,
должна выковырять из команды адрес данных и загрузить их.
Этот вариант медленнее, и команды становятся более длинными, зато данные можно хранить в любом порядке и в любом уголке памяти.
\\ \\
Чтобы калькулятор стал полноценным компьютером, в него ещё нужно добавить команду условного перехода:
возможность по какому-то условию программно изменять значение счётчика.
Добавить её не сложно: просто надо на какой-то свободный выход селектора команд повесить схему,
записывающую новое значение в счётчик.
Я не буду рассматиривать, как именно это сделать --- и условный переход, и всё остальное
намного чётче и подробнее описано в книжке Петцзольда ``Code''.

\section{Ассемблер}
Человеку неудобно работать с битами: трудно запоминать и различать длинные последовательности нулей и единиц.
Куда проще придумать короткое, запоминающееся название для каждой машинной команды.
Язык таких обозначений называется языком ассемблера (сокращённо просто ``ассемблер'').
\\ \\
Программа на ассемблере --- это текст, то есть последовательность символов.
Чтобы исполнить, её нужно ассемблировать --- перевести на машинный язык.
Ассемблирование заключается в тупом сопоставлении каждой текстовой команде её бинарного представления.
\\ \\
Полученная машинная программа --- это последовательность битов.
Это исполняемая программа: её можно загрузить в память, передать на неё управление,
и процессор начнёт декодировать биты и исполнять соответствующие команды.
\\ \\
Можно также провести обратное преобразование --- дизассемблирование,
то есть перевод машинной программы в программу на ассемблере.

\section{Итоги}
Машинная команда --- это набор битов, которым обозначается то или иное действие компьютера.
Когда этот набор битов попадает в виде электрических сигналов на вход декодера команд,
он активирует схему, отвечающую за выполнение этой команды.
Множество машинных команд образуют машинный язык.
\\ \\
Ассемблер --- это язык символьных обозначений машинных команд.



\chapter{Переход к языкам высокого уровня}
Зачем вообще нужны языки высокого уровня?
Люди плохо приспособлены для машинного языка
Когда им приходится одновременно держать в голове много мелочей, они:
\begin{itemize}
\item делают много ошибок
\item пишут неэффективные программы (упускают из виду высокоуровневые оптимизации)
\item не могут быстро читать машинный код (и соответственно вносить изменения)
\end{itemize}
Конечно, есть исключения --- это маленькие, критичные ко времени исполнения куски программы (например, тело цикла, или отдельная функция).
Компилятор не может оптимизировать такой кусок лучше, чем человек.
Такие куски программы иногда пишут в виде ассемблерных вставок.
\\ \\
Но писать большую программу на ассемблере --- значит обрекать её на непортабельность, неподдерживаемость,
и скорее всего некорректность и неэффективность.
\\ \\
Итак, языки высокого уровня нужны, а значит, нужно что-то, что умело бы исполнять программы на этих языках.
Можно было бы сделать специальный процессор для высокоуровневого языка,
но такой процессор был бы одновременно очень сложным и узкоспециализированным,
его трудно было бы менять под нужды программистов (а нужды постоянно меняются).
Поэтому высокоуровневые языки реализуются не в железе, а программно.
\\ \\
Есть два основных подхода к реализации языка программирования: интерпретация и компиляция.
\\ \\
\begin{minipage}{0.4\textwidth}
\includegraphics[width=\textwidth]{pic/interpreter_compiler.png}
\end{minipage}
\begin{minipage}{0.6\textwidth}
Интерпретация состоит из анализа, (возможно) оптимизации и исполнения.
Интерпретатор принимает на вход программу и входные данные для этой программы,
а на выходе выдаёт результат исполнения программы.
\\ \\
Компиляция состоит из анализа, (возможно) оптимизации и синтеза (то есть генерации кода).
Компилятор принимает на вход программу на исходном языке, а выдаёт программу на целевом языке.
Целевой язык совсем не обязан быть машинным, но нас интересует именно этот случай
(иначе компилятор --- только звено в цепи преобразований программы на пути к её исполнению),
поэтому дальше я буду подразумевать, что целевой язык --- машинный.
\\ \\
Начальные фазы (анализ и оптимизация) --- общие для интерпретаторов и компиляторов
(хотя есть оптимизации, специфичные для одного из двух подходов).
\end{minipage}
\\ \\
Все остальные подходы --- это в том или ином виде комбинация этих двух.
Общего красивого названия для интерпретаторов и компиляторов я не знаю, буду называть их \emph{языковыми процессорами}.

\section{Анализ}
Анализ программы заключается в том, что языковой процессор читает исходный код
и восстанавливает по нему структуру программы (как её представляет себе человек).
Получается, исходный код --- это общий язык программиста и компьютера.
Если бы компьютер понимал мысли программиста, анализ был бы не нужен.
\\ \\
Анализ программы состоит из двух этапов: синтаксический анализ и семантический анализ.

\subsubsection{Синтаксический анализ}
Синтаксический анализ --- это преобразование программы-строки в программу-структуру в соответствии с синтаксической грамматикой языка.
\\ \\
Часто грамматика языка разделена на лексическую и синтаксическую.
В этом случае лексический анализ выделяется в отдельную фазу:
лексический анализатор преобразует исходный код в последовательность \emph{токенов} --- пар <тип лексемы, значение лексемы>.
Синтаксический анализатор работает уже не с отдельными символами, а с токенами.
\\ \\
Синтаксический анализатор напрямую зависит от лексического анализатора.
Обратная зависимость в некоторых случаях тоже имеет место:
некоторые лексемы могут означать совершенно разные вещи в зависимости от синтаксического контекста.
\\ \\
Лексическая грамматика обычно регулярная, синтаксическая --- контекстно-свободная.

\subsubsection{Семантический анализ}
Семантический анализатор проверяет структуру, распознанную синтаксическим анализатором, на предмет семантических ошибок.
По-хорошему, синтаксическая грамматика языка не должна порождать противоречивые или бессмысленные конструкции.
Более того, если она их порождает, то всегда можно составить более точную грамматику.
Беда в том, что точная грамматика будет слишком сложной:
если язык Тьюринг-полный, то его бесконечно точная грамматика, полностью выражающая семантику, будет Тьюринг-полной (то есть неограниченной).
От такой грамматики компьютер может треснуть,
поэтому синтаксис приходится умышленно огрублять, а всякие тонкости спихивать на семантику.
\\ \\
Семантический анализ обычно размазан по разным фазам языкового процессора.
Хороший пример семантического анализа --- это проверка типов:
в статически типизированных языках типы проверяются на этапе анализа,
в динамически типизированных --- на этапе исполнения.

\section{Оптимизация}
Оптимизация --- это преобразование программы, которое семантически не изменяет программу,
но уменьшает количество потребляемых ресурсов (время, память и т.д.).
Оптимизация не делает программу оптимальной, просто делает её лучше.
\\ \\
Оптимизацию можно проводить на разных уровнях.
\begin{itemize}
\item Оптимизации программиста:
    \begin{itemize}
    \item на уровне архитектуры программы (т.е. взаимодействия разных её компонент)
    \item на уровне алгоритмов решения отдельных подзадач
    \item на уровне исходного кода
    \end{itemize}
\item Оптимизации языкового процессора:
    \begin{itemize}
    \item на уровне промежуточного представления
    \item на уровне машинных инструкций (компиляторы)
    \item на уровне входных данных (интерпретаторы)
    \end{itemize}
\end{itemize}
Кроме того, оптимизации можно поделить по ширина охвата:
\begin{itemize}
\item глобальные --- касаются всей программы
\item локальные --- касаются отдельной части программы
\end{itemize}
Чем оптимизация глобальнее и высокоуровневее, тем сильнее она влияет на программу.
\\ \\
С точки зрения програмиста, крайне важно писать программы от большого к малому:
тщательно продумать архитектуру и создать рабочий \emph{прототип} программы,
потом перейти к отдельным подзадачам,
и только в самом конце (когда программа уже работает) заняться мелкими оптимизациями.
Прежде чем внедрять оптимизацию, нужно сначала найти проблемное место,
а потом обязательно убедиться, что соответствующая оптимизация делает лучше.
\textbf{Это очень важно.}
Бесполезные оптимизации запутывают исходный код и вносят ошибки.
\\ \\
Языковой процессор знает о программе куда меньше программиста, поэтому он вообще не способен проводить высокоуровневые оптимизации.
Зато он куда лучше программиста умеет ковыряться в деталях и проверять множество случаев,
поэтому низкоуровневые оптимизации --- его удел, и программисту стоит соваться в них только в крайнем случае.
Некоторые языковые процессоры могут проводить и довольно сильные оптимизации
на уровне всей программы (параллелизм, анализ потока данных).
\\ \\
Я не буду подробно рассказывать об оптимизациях --- это отдельная серьёзная и интересная тема, и я в ней не шарю.
Скажу только, что их очень много и они очень разные.

\section{Промежуточные представления}
И компиляторы, и интерпретаторы могут использовать ноль или более \emph{промежуточных представлений}.
Промежуточное представление --- это некое эквивалентное представление исходной программы,
в котором языковой процессор хранит программу.
Промежуточные представления могут быть очень разными: абстракное синтаксическое дерево,
байткод какой-нибудь виртуальной машины, более простой язык --- что угодно.
\\ \\
В совсем простых случаях можно вообще обойтись без промежуточного представления:
читать кусок за куском код программы, анализировать его и тут же исполнять или генерировать инструкции.
Такой подход возможен, если в каждой конкретной точке программа зависит только от того, что было,
последующие части программы не влияют на предыдущие.
\\ \\
В более сложных случаях промежуточное представление необходимо:
исполнение/генерация кода и некоторые фазы анализа требуют информации обо всей программе.
Например, во многих языках программирования объявление функции может быть отвязано от её определения,
поэтому вызов функции может встретиться в программе после объявления, но до определения.
Другой пример: вывод типов на основании всех известных в программе типов.
\\ \\
Даже в простых случаях промежуточное представление может быть очень удобно,
в частности для проведения глобальных оптимизаций.

\section{Интерпретаторы}
Поскольку результат работы интерпретатора --- это исполнение программы для конкретных входных данных,
то результат этот ``одноразовый'' --- его невозможно сохранить для последующего использования.
Программы на интерпретируемых языках хранятся и распространяются в виде исходного кода.
Каждый раз интерпретатор заново читает исходный код, анализирует его, (возможно) проводит оптимизации
и исполняет.
\\ \\
Большой недостаток интерпретации --- медленность исполнения программы.
Время исполнения программы включает время интерпретации: анализ и оптимизация каждый раз повторяются заново.
Кроме того, непонятно, сколько времени надо тратить на оптимизацию:
некоторые программы плохо оптимизируются --- их быстрее исполнить ``как есть'',
а иногда стоит помучиться и оптимизировать какой-нибудь горячий кусок ---
время, затраченное на оптимизацию, с лихвой окупится за счёт быстрого исполнения.
Оптимальные затраты на оптимизацию зависят от конкретной программы, их невозможно вычислить заранее.
Можно только попытаться угадать (что и делают интерпретаторы).
\\ \\
Большое достоинство интерпретации --- портабельность.
Чтобы портировать интерпретатор на какую-то архитектуру, не нужно менять ничего в самом интерпретаторе.
Достаточно, чтобы язык, на котором написан интерпретатор, был портирован на эту архитектуру.
Как только интерпретатор запустился на новой архитектуре, любая программа может быть исполнена на нём.
\\ \\
Другое достоинство интерпретации --- гибкость.
Во-первых, оптимизации можно проводить с учётом конкретных входных данных.
Во-вторых, появляется ``доступ к кишкам'' интерпретатора:
поскольку программу невозможно исполнить без интерпретатора, прямо из программы можно дёргать интерпретатор.
Можно на ходу состряпать кусок кода на исходном языке и тут же его исполнить,
то есть можно во время исполнения менять саму программу.
\\ \\
Наконец, ещё одно достоинство интерпретации --- простота: не нужно ничего знать о целевой архитектуре.

\section{Компиляторы}
Компилятор транслирует программу на исходном языке в программу на целевом языке.
(Я уже говорила, что целевой язык не обязательно машинный, но я рассматриваю
компилятор как единственное и самодостаточное средство преобразования программы).
Программы на компилируемых языках могут храниться и распространяться как в виде исходного кода,
так и в виде целевого кода (исполняемых файлов).
\\ \\
Большое достоинство компиляции --- быстрота исполнения программы.
Исполнение отвязано от компиляции: один раз скомпилировал --- исполняй сколько угодно раз,
поэтому время исполнение не включает время компиляции.
При компиляции можно тратить много времени на оптимизацию.
Когда-нибудь потом готовая, оптимизированная до зубов программа быстренько исполнится.
Такой подход порождает чёткое разделение статического и динамического в программе:
всё статическое вычисляется во время компиляции и не влияет на время исполнения.
\\ \\
Большой недостаток компиляции --- непортабельность.
В отличие от интерпретатора, компилятор много возится с особенностями целевой архитектуры.
Поэтому чтобы портировать его на новую архитектуру, нужно полностью переделать фазу синтеза и связанные с ней оптимизации.
Сколько архитектур --- столько разных фаз синтеза, и за всеми нужно следить, чтобы они генерировали правильный и быстрый код.
\\ \\
Другой недостаток компиляции --- негибкость.
Один раз скомпилированная программа используется сразу для всех входных данных, не оптимизируется для каждого конкретного случая.
В компилируемых языках обычно отсутствуют \texttt{eval}-оподобные возможности:
исполнение программы не требует компилятора, поэтому у программы нет доступа к нему
(ну разве что включать компилятор в виде библиотеки, от чего теряется весь смысл компиляции).
\\ \\
Ну и ещё один недостаток компиляции --- сложность (связанная с поддержкой разных архитектур).

\section{Bootstrap}
И компилятор, и интерпретатор --- это просто программа.
Как и любая программа, она написана на каком-то языке программирования. На каком?
\\ \\
Если представить, что в мире нет ни одного высокоуровневого языка --- то вариантов нет,
любой языковой процессор должен быть написан на машинном языке.
Однако как только появился хоть один высокоуровневый язык, на нём можно писать языковые процессоры для других языков.
(Разумеется, язык должен быть достаточно выразительным  ---
некоторые языки специального назначения плохо подходят для таких задач.)
\\ \\
Когда у языка есть хотя бы одна реализация, можно писать вторую --- но уже на этом же самом языке.
Язык, у которого языковой процессор написан на нём самом, называется \emph{self-hosting}.
Поначалу это сбивает с толку, но нужно помнить, что никакой магии нет:
у истоков любого высокоуровневого языка стоял другой язык, а у истоков всех языков стоял машинный.
Бытует мнение, что self-hosting --- своего рода ``тест на вшивость'' языка:
если язык позволяет реализовать самого себя --- значит, он не так плох.
Но я думаю, что дело тут в другом: разработчикам self-hosting языка постоянно приходится
чувствовать себя в шкуре программистов, и они лучше видят проблемы.
Сам процесс создания self-hosting языкового процессора называется \emph{bootstrap}'ом.
\\ \\
С компиляторами всё просто.
Первая версия компилятора пишется на каком-то уже реализованном языке.
Эта версия обычно ограниченная, поддерживает только подмножество языка, достаточное для создания второй версии компилятора.
Вторая версия пишется уже на новом языке и компилируется при помощи первой.
После этого первую версию можно забыть и выбросить, а компилятор с этого момента действительно самодостаточен.
Примеры языков, у которых есть self-hosting компилятор: C/C++, Haskell, Rust.
\\ \\
С интерпретаторами всё не так.
Когда написана первая версия на чужом языке, можно написать вторую версию на интерпретируемом языке.
Но вот выкинуть первую версию нельзя --- вторую будет просто негде запустить.
Поэтому интерпретатор вынужден всегда таскать за собой самую первую версию.
Можно представить себе стек версий: первая запускает вторую, вторая --- третью и так далее,
пока не запустится самая новая версия, которая наконец соизволит запустить программу.
Это ``малость'' неэффективно и очень хрупко --- наслаиваются ошибки из разных версий.
Поэтому на практике изворачиваются кто как может (например, пишут компилятор для маленького подмножества языка,
а интерпретатор пишут на этом подмножестве и компилируют).
Примеры языков, у которых есть self-hosting интерпретатор: Basic, Lisp, Prolog, Python.

\section{JIT-компиляторы}
Грубо говоря, компиляция --- быстрый, но непортабельный и негибкий подход,
а интерпретация --- портабельный и гибкий, но медленный.
Некоторые люди готовы всем пожертвовать ради скорости (например я), некоторые предпочитают гибкость или портабельность,
а некоторые попытались найти компромисс --- JIT-компиляторы.
\\
\includegraphics[height=3.5in]{pic/jit.png}
\\
JIT (Just In Time) компиляторы --- это языковые процессоры, которые компилируют программу на ходу и тут же исполняют.
Обычно JIT-компилятор встроен в интерпретатор: интерпретатор на ходу определяет ``горячие'' куски программы, компилирует их JIT-ом и тут же исполняют.
Правильнее было бы разграничивать интерпретатор и встроенный в него JIT,
но применяются JIT-ы в основном как навороченная оптимизация в интерпретаторах,
поэтому я под JIT-компиляцией буду подразумевать ``интерпретатор + JIT''.
\\ \\
Основная цель JIT-компиляторов --- разогнать медленный интерпретатор.
Скорость исполнения программ сильно зависит от конкретной программы:
если в ней много длинных циклов, JIT-компилятор может быть даже быстрее компилятора
(за счёт оптимизаций, специфичных для конкретных входных данных).
Если в программе нет ``горячих'' кусков (время исполнения равномерно размазано по всей программе)
или если программа сильно меняется на ходу, то JIT-компилятор будет даже медленнее интерпретатора
(за счёт дополнительных затрат на компиляцию).
В целом JIT может на порядки разогнать интерпретатор для некоторых типичных программ.
\\ \\
С портабельностью та же беда, что и с компиляторами: тяжело даётся поддержка каждой новой архитектуры.
Правда, если JIT не поддерживает какую-то архитектуру, есть запасной вариант --- всё исполнять на интерпретаторе.
Такая, неэффективная, портабельность есть.
Сами программы не нужно перекомпилировать --- они распространяются в виде исходного кода и компилируются на ходу.
\\ \\
Гибкость интерпретаторов остаётся,
но добавляется новая головная боль: если программа на ходу изменилась,
то скомпилированный код мог устареть, и нужно перекомпилировать его заново.
\\ \\
Чего точно нет в JIT-компиляторах --- так это простоты.
К сложности компиляции добавляется ещё и необходимость эвристически определять,
какие куски программы нужно на ходу скомпилировать, и следить за изменениями программы.
\\ \\
Примеры JIT-компиляторов: V8 (Javascript), PyPy (Python), LuaJIT (Lua).

\section{Виртуальные машины}
Другая попытка компромисса между компиляторами и интерпретаторами --- виртуальные машины.
Виртуальная машина --- это эмулятор одного исполнителя на другом исполнителе.
\\ \\
Языковой процессор разбивается на два этапа.
Первый этап --- это компиляция исходного кода в язык, понимаемый виртуальной машиной --- \emph{байткод}.
Второй этап --- это интерпретация байткода виртуальной машиной (это может быть простая интерпретация, JIT-компиляция
или любая цепочка преобразований байткода, которая в конечном счёте приведёт к исполнению программы).
\\
\includegraphics[height=3.5in]{pic/vm.png}
\\
Во-первых, такой подход позволяет на первом этапе сделать значительную часть работы --- анализ и оптимизацию исходного кода.
Время исполнения программы включает только второй этап: интерпретацию байткода,
а это значительно проще, чем интерпретация исходного кода.
\\ \\
Во-вторых, байткод виртуальной машины платформенно-независим:
его можно исполнять на любой платформе, где есть эта виртуальная машина.
Поэтому программы в виде байткода портабельны.
\\ \\
В-третьих, виртуальная машина по-прежнему может оптимизировать байткод для конкретных входных данных.
С изменениями программы во время исполнения уже не так хорошо:
виртуальная машина не понимает исходный код, она понимает только байткод.
Поэтому программы можно менять, но на уровне байткода.
\\ \\
Виртуальная машина не привязана к исходному языку: она вообще ничего о нём не знает.
Поэтому ничто не мешает любому языку компилироваться в байткод виртуальной машины и спокойно паразитировать на ней.
Это очень полезное свойство, как для пользоваетелей виртуальной машины
(они могут сэкономить усилия при создании языковых процессоров),
так и для разработчиков виртуальной машины (много кто заинтересован в её качестве).
Главное при этом, чтобы байткод виртуальной машины был стабильным и имел чёткую спецификацю.
\\ \\
Канонический пример виртуальной машины --- JVM (Java Virtual Machine).

\section{Суперкомпиляторы}
Суперкомпиляция, несмотря на многообещающее название, --- это вовсе не архи-хорошая-компиляция, а просто один из способов оптимизации.
Суперкомпиляцию придумал В.Ф. Турчин, и вот как объясняет это слово один из его последователей, Сергей Романенко:
\\ \\
\begin{footnotesize}
\emph{
Сам термин ``суперкомпиляция'', может быть, и не очень хорош в силу своей двусмысленности.
``Супер'' может означать ``крутой и могучий'' (``супермен'' = ``сверхчеловек''),
а может означать ``тот, кто находится сверху и присматривает'' (``супервизор'' = ``надсмотрщик'').
Когда придумывался термин ``суперкомпилятор'' имелась в виду не ``могучесть'', а ``присмотр''...
}
\end{footnotesize}
\\ \\
Я упоминаю суперкомпиляцию в основном чтобы убрать путаницу, связнную с этим понятием,
хотя и сама техника стоит того, чтобы о ней знать.
\\ \\
Суперкомпиляция заключается примерно в следующем.
Делается попытка выполнить программу, но не для конкретных данных, а для любых, абстрактных данных (или данных, удовлетворяющих специальным ограничениям).
При этом строится \emph{дерево конфигураций}, где конфигурации --- это состояния программы на разных этапах исполнения.
Если программа содержит циклы или рекурсию, то дерево конфигураций может получиться бесконечным.
Чтобы этого избежать, в дереве ищутся повторяющиеся конфигурации и дерево свёртываются в \emph{граф конфигураций}.
По графу конфигураций восстанавливается \emph{остаточная} программа --- образ исходной программы с учётом входных данных.
В остаточной программе отстутствуют бесполезные части исходной программы (недостижимые для входных данных).
\\ \\
Идея суперкомпиляции очень сильная, но подход не лишён практических трудностей
(самая большая проблема --- остаточная программа может быть слишком большой).
Поэтому пока что суперкомпиляция используется редко.
Были попытки писать суперкомпиляторы для разных языков: Рефал (язык, придуманный Турчином), Haskell, Scala, даже Java.

\section{Итоги}
Есть два основных подхода к реализации высокоуровневых языков программирования: интерпретация и компиляция.
Основной недостаток интерпретации --- медленность,
компиляции --- трудность портирования на разные архитектуры и невозможность менять программу на ходу (на уровне исходного кода).
Есть две основных попытки компромисса между компиляцией и интерпретацией: JIT-компиляция и виртуальные машины.
\\ \\
В реальной жизни вид языкового процессора определяется языком.
Если главная цель языка --- скорость исполнения программ, то почти наверняка у него будет компилятор.
Если цели другие (гибкость, возможность изменять программы на ходу) --- то скорее всего понадобится интерпретатор.
Часто какой-то язык начинает с медленного интерпретатора, который потом обрастает оптимизациями и превращается в JIT.
Иногда встречаются настоящие кадавры, состоящие из нескольких уровней интерпретаторов, JIT-компиляторов и виртуальных машин.
\\ \\
Прочитай короткую захватывающую статью с картинками ``The Three Projections of Doctor Futamura''. :)



\chapter{Детсадовский пример}
\definecolor{cgray}{rgb}{0.96, 0.95, 0.9}
\definecolor{cblue1}{rgb}{0.0, 0.15, 0.3}
\definecolor{cblue2}{rgb}{0.0, 0.4, 0.6}
\definecolor{cblue3}{rgb}{0.0, 0.7, 1.0}
\definecolor{cgreen}{rgb}{0.2, 0.6, 0.0}
\lstdefinestyle{src}
    { numbers=left
    , numberstyle=\footnotesize\ttfamily\color{cblue3}
    , breaklines=true
    , basicstyle=\footnotesize\ttfamily\color{cblue1}
    , keywordstyle=\footnotesize\ttfamily\color{cblue2}
    , commentstyle=\itshape\color{cgreen}
    , backgroundcolor=\color{cgray}
    }
\lstset{style=src}
Возьмём игрушечный язык и построим для него несколько примеров языковых процессоров:
\begin{enumerate}
\item Интерпретатор
\item Компилятор
\item Виртуальную машину
\item Интерпретатор байткода виртуальной машины
\item JIT-компилятор байткода виртуальной машины
\end{enumerate}
\includegraphics[width=4in]{pic/scheme.png}
\\
Все языковые процессоры будут использовать общий парсер.
Парсер будет генерировать AST.
Каждый языковой процессор будет рекурсивно обходить AST и делать что-то своё:
выполнять программу, генерировать машинные инструкции, генерировать байткод виртуальной машины.
Чего в нашем примере не будет --- так это оптимизаций (слишком простой язык).

\section{Язык}
Язык я выберу очень простой: арифметические операции над целыми числами в десятичном представлении.
Синтаксис нашего языка можно описать такой грамматикой $G=<\Sigma_T,\Sigma_N,P,S>$:
$$\Sigma_T=\{0,1,2,3,4,5,6,7,8,9,+,-,*,/,(,)\}$$
$$\Sigma_N=\{digit, number, factor, term, expr\}$$
$$P=\left\{
\begin{array}{l l}
digit \  & \rightarrow \ 0 \ | \ 1 \ | \ 2 \ | \ 3 \ | \ 4 \ | \ 5 \ | \ 6 \ | \ 7 \ | \ 8 \ | \ 9
\\
number \  & \rightarrow \ digit \ number \ | \ digit
\\
factor \  & \rightarrow \ number \ | \ ( expr )
\\
term \  & \rightarrow \ term * factor \ | \ term / factor \ | \ factor
\\
expr \  & \rightarrow \ expr + term \ | \ expr - term \ | \ term
\end{array}
\right. $$
$$S=expr$$
\\
Эта грамматика может показаться сложноватой.
Во-первых, вместо трёх правил для нетерминалов $factor$, $term$ и $expr$ можно обойтись
одним:
$$expr \  \rightarrow \ expr + expr \ | \ expr - expr \ | \ expr * expr \ | \ expr / expr \ | \ ( expr ) \ | \ number$$
\\
Это правило позволяет описывать те же самые языковые конструкции, что и три правила $factor$, $term$ и $expr$ вместе взятые.
Кроме того, оно проще и очевиднее. Зачем разбивать его на три? А вот зачем:
такое разделение позволяет прямо в грамматике закодировать \emph{приоритеты} арифметических операций.
У скобок самый большой приоритет (правило $factor$); затем идут мультипликативные операции с меньшим приоритетом (правило $term$);
затем аддитивные с ещё меньшим приоритетом (правило $expr$). Получается, что трёхуровневые правила
описывают тот же \emph{синтаксис}, что и одноуровневое, но при этом описывают часть \emph{семантики} языка
(приоритет арифметических операций).
\\ \\
Вторая странность нашей грамматики, это то, что правила для арифметических операций имеют вид
$a \rightarrow a \circ b$. Возьмём, например, правило для сложения:
$expr \rightarrow expr + term$. Почему не $expr \rightarrow term + expr$ ? Или самый очевидный
вариант: $expr \rightarrow expr + expr$ ? Все три варианта описывают синтаксически эквивалентные языковые конструкции.
Чем первый вариант лучше? Дело тут в другой семантической особенности
нашего языка: \emph{левоассоциативность} бинарных операторов вычитания и деления.
Выражение ``$1 - 2 - 3$'', к примеру, следует понимать как ``$(1 - 2) - 3$'', а не ``$1 - (2 - 3)$''.
Рассмотрим повнимательнее правило для вычитания: $expr \rightarrow expr - term$.
Слева от ``$-$'' стоит $expr$, т.е. любое выражение. А вот справа --- $term$,
т.е. выражение, допускающее только мультипликативные операции над числами и выражениями в скобках.
Получается, такое правило заставляет нас однозначно понимать ``$1 - 2 - 3$'' как ``$(1 - 2) - 3$''.
Если бы мы выбрали правило $expr \rightarrow term - expr$, мы бы получили \emph{правоассоциативный}
оператор вычитания (и такие операторы иногда нужны). А вот третье правило, $expr \rightarrow expr - expr$,
вообще \emph{неоднозначное}: оно позволяет понимать выражение ``$1 - 2 - 3$''
двумя способами. Неоднозначность в грамматиках --- источних бед. Её следует допускать только тогда, когда
сам по себе язык неоднозначен (как, например, русская фраза ``он умеет заставить себя слушать'').
Кстати, правила для сложения и умножения могут быть как левоассоциативными, так и правоассоциативными, лишь бы не неоднозначными.
\\ \\
Семантика нашего языка очень проста: это семантика арифметических выражений.
По-хорошему, я должна была бы формально определить её, но сделаю вид, что и так понятно.
Кстати, из-за этого потом начнутся беды: я втихаря ограничу размер чисел 32-мя битами, а ты ничего не заметишь. ;)

\section{Парсер}
Раз уж мы собрались строить языковые процессоры для языка арифметических
выражений, то начинать нужно с парсера. Парсер должен структурировать входную
строку в соответствии с нашей грамматикой:
\\ \\
\includegraphics[height=3.5in]{pic/18.png}
\\ \\
Дерево на картинке --- это \emph{дерево разбора}.
Вообще, правильнее было бы говорить \emph{граф разбора}, потому что этот граф является деревом только для некоторых классов грамматик.
В нашем случае мы с полным правом можем говорить ``дерево''.
Возникает два вопроса:
\begin{enumerate}
\item Как заставить парсер догадаться, какая стуктура соответствует строке?
Явно придётся выковыривать символы, искать среди них плюсы, минусы и прочее.
Но как сделать это грамотно?
\item Допустим, парсеру каким-то чудом удалось распознать в строке
структуру. Что он должен делать с этим тайным знанием? Немедленно использовать
и тут же забыть? Или как-то сохранить для последующего использования?
\end{enumerate}
Разберёмся сначала с первым.
\\ \\
Один из самых простых подходов при написании парсера --- это так называемый \emph{метод
рекурсивного спуска} (recursive descent). Суть метода такая: для каждого нетерминала пишется
отдельная маленькая функция-парсер. Эта функция вызывает функции-парсеры для других нетерминалов (возможно, саму себя) ---
точно так же, как правила грамматики ссылаются друг на друга. В итоге
парсер структурно повторяет грамматику. Примерно так:

\lstinputlisting[language=Haskell]{example_recursive_descent/parser1_bad.hs}
Парсер написан на языке Haskell с использованием библиотеки Parsec.
Как видно из 1-й строки, мы взяли всего три библиотечных функции: \texttt{<|>}, \texttt{char} и \texttt{parse}.
Остальное --- встроенные хаскельные функции и операторы.
Оператор \texttt{<|>} работает как логическое \emph{или}: запускает левый парсер,
и если он отработал неудачно, запускает правый парсер.
Оператор \texttt{>\thinspace >} работает как логическое \emph{и}: запускает левый парсер,
и если он отработал удачно, запускает правый парсер.
Поскольку наш парсер пока не возвращает никакого результата (только проверяет синтаксическую корректность),
то все функции-парсеры должны возвращать значение типа \texttt{()} 
(аналог типа \texttt{void} в С/С++). Чтобы возвращать \texttt{()},
функция должна конце вызывать \texttt{return ()} или другую функцию, которая возвращает \texttt{()}.
\\ \\
Метод рекурсивного спуска --- это очень простой метод
построения парсеров, наиболее \emph{естественный} для человека: он позволяет писать парсер почти не задумываясь,
просто глядя на грамматику. На практике у этого метода есть две основные проблемы.
\\ \\
Первая проблема --- корректный перебор нескольких альтернатив. Если не сработал первый парсер, надо не просто
запустить второй, но ещё и перед этим аккуратно откатить назад все изменения, сделанные первым (backtrack).
В частности, если первый парсер продвинулся на несколько символов во входной строке,
то надо вернуться на прежнюю позицию. В библиотеке Parsec для этого есть функция \texttt{try}:
она запоминает состояние, к которому нужно откатиться.
С учётом функции \texttt{try} парсер выглядит так:
\lstinputlisting[language=Haskell]{example_recursive_descent/parser2_better.hs}
Обрати внимание, что \texttt{try} нужно вставлять перед всеми альтернативами,
кроме последней: если она не сработает, весь парсер не сработает. Кроме того, \texttt{try}
не надо вставлять, если альтернатива проверяет не больше одного символа.
\\ \\
Вторая проблема --- левая рекурсия в грамматике. Например, правило $expr \rightarrow expr - term$ леворекурсивное,
потому что крайний левый символ правой части --- это сам определяемый нетерминал.
Понятно, что если функция-парсер \texttt{expr} первым делом начнёт
вызывать саму себя (что она и делает), то программа в лучшем случае зациклится (а в худшем сожрёт память и упадёт).
Есть два пути решения этой проблемы:
    \begin{enumerate}
    \item Перестраивать грамматику при помощи формального алгоритма избавления от левой рекурсии.
    Этот подход гарантирует, что порождаемый грамматикой язык не изменится, и что левой рекурсии
    не будет. Из недостатков --- грамматика станет не такой красивой и удобной. Зато этот алгоритм
    универсальный.
    \item Вставлять костыли в парсер. В простых случаях удаётся ``на глаз'' переписать функцию-парсер так,
    чтобы она по смыслу делала то же самое, но не зацикливалась. В общем случае есть формальные
    алгоритмы, позволяющие грубой силой или хитростью ограничивать левую рекурсию, но эти
    алгоритмы довольно сложные и кривые.
    \end{enumerate}
Пойдём первым путём: применим к грамматике формальный алгоритм устранения левой рекурсии.
Наш случай простой: мы имеем дело с
\emph{непосредственной} левой рекурсией, т.е. правилом вида $A \rightarrow A \alpha$.
(Бывает ещё \emph{косвенная} левая рекурсия, образованная не одним, а несколькими правилами:
например $A \rightarrow B \alpha$, $B \rightarrow A \beta$.) Алгоритм избавления от непосредственной
левой рекурсии очень простой: каждое леворекурсивное правило $A \rightarrow A \alpha_1 | ... | A \alpha_n | \beta_1 | ... | \beta_m$
заменяется парой правил $A \rightarrow \beta_1 A_1 | ... | \beta_m A_1$,
$A_1 \rightarrow \alpha_1 A_1 | ... | \alpha_n A_1 | \epsilon$, где $A_1$ --- новый нетерминал.
В нашей грамматике леворекурсивных правила два: $term$ и $expr$. Изменённая грамматика
имеет вид:

$$\Sigma_T=\{0,1,2,3,4,5,6,7,8,9,+,-,*,/,(,)\}$$
$$\Sigma_N=\{digit, number, factor, term, term_1, expr, expr_1\}$$
$$P=\left\{
\begin{array}{l l}
digit \  & \rightarrow \ 0 \ | \ 1 \ | \ 2 \ | \ 3 \ | \ 4 \ | \ 5 \ | \ 6 \ | \ 7 \ | \ 8 \ | \ 9
\\
number \  & \rightarrow \ digit \ number \ | \ digit
\\
factor \  & \rightarrow \ number \ | \ ( expr )
\\
term \  & \rightarrow \ factor \ term_1
\\
term_1 \  & \rightarrow \ * factor \ term_1 \ | \ / factor \ term_1 \ | \ \epsilon
\\
expr \  & \rightarrow \ term \ expr_1
\\
expr_1 \  & \rightarrow \ + term \ expr_1 \ | \ - term \ expr_1 \ | \ \epsilon
\end{array}
\right. $$
$$S=expr$$
\\
Алгоритм гарантирует, что новая грамматика порождает в точности тот же язык,
что и старая (в том числе сохранилась семантика приоритетов и ассоциативности). Парсер для новой грамматики
отличается от старого парсера \emph{точно так же}, как новая грамматика отличается от старой:

\lstinputlisting[language=Haskell]{example_recursive_descent/parser3_best.hs}
Как хорошо! Все изменения сделаны автоматически, поэтому ошибки маловероятны.
Если б мы попытались переписать парсер руками, пришлось бы думать и хитрить, и
не было бы этой замечательной уверенности, что всё сделано правильно.
Мы сохранили главное: близость парсера к грамматике. До тех пор, пока парсер
соответствует грамматике, он остаётся \emph{понятным} и его \emph{легко менять},
изменяя  грамматику. Но как только парсер отдалится от грамматики,
он навсегда погрязнет в болоте сомнительных костылей и чудом работающих хаков.
\\ \\
На этом с синтаксическим разбором всё: у нас есть алгоритм написания парсера (метод рекурсивного спуска)
и грамматика, хорошо подходящая для этого алгоритма. Дальше по этой теме читай в книжке Grune, Jacobs: ``Parsing Technique. A Practical Guide'' --- там
приводится полная классификация алгоритмов синтаксического разбора со всеми их слабыми и сильными местами.
\\ \\
Теперь вернёмся ко второму вопросу: в каком виде парсер должен возвращать результат?
Очевидно, это зависит от того, что мы понимаем под результатом. Если нужно просто
проверить синтаксическую корректность выражения, можно ничего не возвращать.
Если мы хотим \emph{вычислить} значение выражения, то придётся следить
за частично вычисленным значением. Если мы хотим \emph{скомпилировать} выражение
в программу на другом языке (или в байткод виртуальной машины), нужно таскать за собой
буфер и дописывать туда новые инструкции. В общем, в каждом конкретном случае
нужно что-то своё.
\\ \\
Можно написать по отдельному парсеру на каждый случай. Но это немного обидно:
самая сложная часть парсера (синтаксический разбор) будет везде повторяться.
Чтобы этого избежать, мы введём промежуточное представление: AST. Парсер будет
строить AST, а потом уже с этим AST можно делать всё, что угодно:
вычислять значение выражения, генерировать инструкции, проводить сложные оптимизации и т.д.
(Важное замечание: поскольку наш язык очень простой, он допускает \emph{прямую} интерпретацию
и компиляцию: прямо по ходу синтаксического разбора, из парсера, можно сделать
всё, что надо. Промежуточное представление для нас --- просто удобный способ не дублировать код.
В более сложных случаях без него не обойтись.)
\\ \\
Какой должна быть структура AST для нашего языка?
Самый простой вариант --- дерево разбора. В начале главы мы уже смотрели на дерево разбора
для выражения $12 + 4 * 3 / (11 - 8)$. Вот как оно выглядит
после избавления от левой рекурсии в грамматике:
\\ \\
\includegraphics[height=3.5in]{pic/19.png}
\\ \\
Дерево разбора представляет структуру арифметического выражения в точном соответствии с грамматикой.
Это дерево хорошее и правильное, но какое-то оно \emph{слишком детальное}.
Например, чтобы добраться до числа $12$, надо обойти вершины $expr$, $term \ expr_1$, $factor \ term_1$ и $number$,
а потом ещё по кускам собирать само число из цифр, сидящих в вершинах $digit$.
Зачем нам хранить в AST информацию, что число $12$ получилось таким хитрым путём?
Ненужная детализация --- это источник бед. Во-первых, она дурит голову (например,
я размазываю простенький пример на десять страниц, и ты не улавливаешь сути). Во-вторых,
все эти детали надо где-то хранить (для больших выражений потребуется уйма памяти).
В-третьих, обходя огромное дерево, больше шансов где-то сделать ошибку.
\\ \\
Человек представляет себе выражение $12 + 4 * 3 / (11 - 8)$ вот так:
\\ \\
\includegraphics[height=2in]{pic/20.png}
\\ \\
Это дерево отражает структуру выражения ничуть не хуже,
чем дерево разбора, но при этом оно намного проще и компактнее. Почему мы не можем
сделать дерево разбора таким же простым? Дерево разбора зависит только от грамматики.
Чтобы дерево разбора было красивым, нужна другая, красивая грамматика. К несчастью,
красивые грамматики обычно напичканы неоднозначностями, и парсер для них написать
очень трудно или вообще невозможно (кроме того, такие парсеры работают очень медленно).
Наша грамматика некрасивая, потому что она приспособлена для компьютера, а не для человека.
Но никто не мешает нам сделать AST не таким, как дерево разбора.
\\ \\
Итак, в нашем AST будут узлы пяти типов: ``число'', ``плюс'', ``минус'', ``умножить'', ``поделить''.
Узел типа ``число'' всегда будет листом, т.е. у него не будет детей. У остальных типов
узлов будет ровно по два ребёнка: левый и правый операнды. Узлы-операнды могут быть числом
(если тип узла - ``число'') или корнем поддерева (если тип узла --- ``плюс'', ``минус'', ``умножить'' или ``поделить'').
На хаскеле такой тип данных выражается проще некуда:
\begin{verbatim}
data AST
    = Number Int
    | Add AST AST
    | Sub AST AST
    | Mul AST AST
    | Div AST AST
\end{verbatim}
Здесь \texttt{AST} --- тип данных, который мы определяем;
\texttt{Int} --- встроенный хаскельный тип; \texttt{Number},
\texttt{Add}, \texttt{Sub}, \texttt{Mul} и \texttt{Div} ---
функции-конструкторы нашего типа \texttt{AST}. Конструктор может принимать аргументы, как и любая функция:
в нашем случае конструктор \texttt{Number} --- это функция, принимающая один аргумент типа \texttt{Int},
а конструкторы \texttt{Add}, \texttt{Sub}, \texttt{Mul} и \texttt{Div} ---
функции, принимающие по два аргумента типа \texttt{AST}. Определения для этих функций писать не надо:
они настолько очевидные, что компилятор сгенерирует их сам.
Теперь сделаем так, чтоб наш парсер сохранял результаты разбора в структуру AST:
\lstinputlisting[language=Haskell]{example_recursive_descent/parser4_ast.hs}
Раньше нас не интересовал результат работы парсера: всё наши
функции-парсеры возвращали значение типа \texttt{()}. Теперь каждая функция-парсер возвращает результат
своей работы: функции \texttt{digit} и \texttt{number} возвращают результат типа \texttt{Int},
а остальные --- результат типа \texttt{AST}. Функции \texttt{expr1} и \texttt{term1}
теперь принимают параметр типа \texttt{AST} --- левый операнд. Затем они пытаются распарсить правый операнд,
и если это удаётся, то из двух операндов строится новый \texttt{AST}, а если нет --- возвращается левый
операнд. Функция \texttt{number} теперь принимает один аргумент типа \texttt{Int} --- частично вычисленное число.
\\ \\
В тех местах, где возвращаемое из парсера значение важно, вместо оператора \texttt{>\thinspace >}
используется очень похожий оператор \texttt{>\thinspace >=}. Отличие этих двух операторов в том, что \texttt{>\thinspace >}
игнорирует значение, возвращаемое первым парсером, а \texttt{>\thinspace >=} передаёт это значение
второму парсеру в качестве аргумента. Иногда надо не просто передать значение второму парсеру,
а как-то более хитро его использовать.
Чтобы как-то назвать, \emph{обозначить} это возвращаемое значение, используется синтаксис \emph{лямбда-функции}:
\texttt{\textbackslash <arguments> -> <calculations with arguments>}.
\\ \\
\texttt{deriving (Show)} (строка 9) говорит компилятору, что надо сгенерировать функции для вывода
значений типа \texttt{AST} на экран. Без этого мы не смогли бы распечатать полученный \texttt{AST}
(строка 44).
\\ \\
Если что-то непонятно в этом исходнике --- не переживай. Здесь уже слишком много хаскельного сиснтакса,
чтобы всё сразу было понятно. Я привожу примеры на хаскеле по двум причинам: во-первых, потому что
на хаскеле они короткие и способствуют пониманию алгоритма; во-вторых, чтоб заинтересовать тебя.
\\ \\
Теперь напишем тот же самый парсер на С++. Вот как выглядит структура \texttt{AST} (файл ast.h):
\lstinputlisting[language=C++]{example_lang_proc/ast.h}
В файле ast.cpp находится реализация конструкторов и деструкторов \texttt{AST}:
\lstinputlisting[language=C++]{example_lang_proc/ast.cpp}
Теперь перейдём к самому парсеру. По сути он мало отличается от хаскельного
парсера, хотя и не такой элегантный. На C++ нам надо вручную следить
за продвижением указателя во входной строке, поэтому мы передаём его первым аргументом
во все функции (в хаскеле это неявно делала библиотека Parsec).
Обрати внимание, что указатель везде передаётся \emph{по ссылке}, поэтому все изменения, которые делает с ним \emph{вызываемая}
функция, становятся видны \emph{вызывающей} функции. Вот прототипы
функций-парсеров (parser.h):
\lstinputlisting[language=C++]{example_lang_proc/parser.h}
А вот их реализации (parser.cpp):
\lstinputlisting[language=C++]{example_lang_proc/parser.cpp}
Вот и всё! Парсер готов. Он, конечно, далеко не идеален (в частности, плохо обрабатывает ошибки).
Но больше мы его менять не будем.

\section{Интерпретатор}
Интерпретатор выглядит до смешного просто. Вот его прототип (файл interpreter.h):
\lstinputlisting[language=C++]{example_lang_proc/interpreter.h}
А вот реализация (файл interpreter.cpp):
\lstinputlisting[language=C++]{example_lang_proc/interpreter.cpp}
Функция \texttt{interpret} определяет тип узла: если это \texttt{NUMBER},
то нужно просто вернуть соответствующее число; иначе нужно рекурсивно вычислить
значение левого и правого операндов и вернуть результат операции над ними.

\section{Компилятор}
Компилятор структурно не сложнее интерпретатора: он точно так же рекурсивно обходит
\texttt{AST}, только вместо немедленного вычисления генерирует машинные инструкции.
Первым делом надо решить важный вопрос: для какой архитектуры?
\\ \\
Архитектура компьютера --- понятие широкое. Нас интересует \emph{архитектура как набор команд процессора} 
(Instruction Set Architecture, ISA). Можно сделать два совершенно
разных процессора, которые будут понимать один и тот же набор команд. Компилятор не увидит
разницы меду ними: ему всё равно, как процессор устроен физически.
Единственное, что компилятору надо знать --- это какие команды понимает процессор
и насколько эффективно он выполняет конкретную команду. Поэтому, говоря об
архитектуре, я имею в виду набор команд процессора.
\\ \\
Какую архитектуру нам выбрать? В принципе, \emph{любую}. Можно, к примеру, выбрать
исторически первый компьютер и компилировать в машинный код для него. 
Только вот запустить такую программу будет негде, поэтому лучше выбрать 
архитектуру твоего компьютера. Кстати, что это за архитектура?
\\ \\
Это можно узнать несколькими способами. Например, поискать в интернетах спецификацию
этой модели (Lenovo ThinkPad X220i). Привожу её тут полностью (вдруг что полезное узнаешь про свой комп),
но нас интересует только секция ``Processor / Chipset'':

\begin{scriptsize}
\begin{verbatim}
Lenovo ThinkPad X220i

    Processor / Chipset
        CPU                                 Intel Core i3 (2nd Gen) 2310M / 2.1 GHz
        Number of Cores                     Dual-Core
        Cache                               L3 - 3 MB
        64-bit Computing                    Yes
        Chipset                             Mobile Intel QM67 Express
        Features                            Intel Turbo Boost Technology 2.0
\end{verbatim}
\end{scriptsize}

\begin{tiny}
\begin{verbatim}
    Memory
        Max RAM Supported                   8 GB
        Technology                          DDR3 SDRAM
        Speed                               1333 MHz / PC3-10600
        Form Factor                         SO DIMM 204-pin
        Slots Qty                           2
        Empty Slots                         1

    Storage
        Interface                           Serial ATA-300

    Display
        LCD Backlight Technology            LED backlight
        Widescreen                          Yes
        Image Aspect Ratio                  16:9
        Features                            anti-glare

    Audio & Video
        Graphics Processor                  Intel HD Graphics 3000
        Memory Allocation Technology        Dynamic Video Memory Technology
        Camera                              Yes
        Resolution                          0.92 Megapixel
        Capture Resolutions                 1280 x 720
        Sound                               Stereo speakers , stereo microphone
        Codec                               CX20672
        Compliant Standards                 High Definition Audio

    Input
        Type                                touch-screen
        Features                            spill-resistant

    Communications
        Wireless                            Bluetooth 3.0
        Wireless Controller                 Intel Centrino Wireless-N 1000
        Network Interface                   Gigabit Ethernet

    Wireless Broadband (WWAN)
        Generation                          3G upgradable

    Battery
        Installed Qty                       1
        Max Supported Qty                   2
        Run Time                            8.8 sec

    AC Adapter
        Input                               AC 120/230 V ( 50/60 Hz )
        Output                              65 Watt

    Connections & Expansion
        Slots                               1 x ExpressCard/54 ( 1 free )
        Interfaces                          2 x USB 2.0
                                            PoweredUSB
                                            VGA
                                            DisplayPort
                                            LAN
                                            Headphone/microphone combo jack
                                            Dock
        Memory Card Reader                  3 in 1 ( SD Card, SDHC Card, SDXC Card )

    Software
        Software Included                   ThinkVantage Toolbox
        Microsoft Office Preloaded          Includes a pre-loaded image of select Microsoft Office
                                            suites. Purchase an Office 2010 Product Key Card or
                                            disc to activate preloaded software on this PC.

    Miscellaneous
        Security                            Trusted Platform Module (TPM 1.2) Security Chip,
                                            fingerprint reader
        Features                            Intel Active Management Technology (iAMT)
        Compliant Standards                 RoHS
        Manufacturer Selling Program        TopSeller

    Dimensions & Weight
        Width                               12 in
        Depth                               9 in
        Height                              1.2 in
        Weight                              3.7 lbs

    Environmental Standards
        ENERGY STAR Qualified               Yes

    Sustainability
        ENERGY STAR Qualified               Yes
        Greenpeace policy rating (Nov 2011) 3.8

    General
        Manufacturer                        Lenovo
\end{verbatim}
\end{tiny}
Видно, что процессор у тебя --- Intel Core i3, двухъядерный и 64-разрядный. Описание
архитектуры этого процессора найти несложно.
\\ \\
Другой, линуксоспецифичный, способ --- выполнить одну из
команд \texttt{arch}, \texttt{uname -m}, \texttt{lscpu}, \texttt{cat /proc/cpuinfo} или ещё какую-нибудь.
Команды \texttt{arch} и \texttt{uname -m} просто бесхитростно пишут название архитектуры (эти команды идентичны).
Команда \texttt{lscpu} выводит больше информации о процессоре,
а команда \texttt{cat /proc/cpuinfo} выводит информацию про каждое ядро в отдельности.
\\ \\
Но что, если ты не знаешь модель (нашла компьютер в канаве), и операционная система на нём
очень глупая? Можно попробовать расковырять исполняемый файл какой-нибудь программы, которая запускается
на этом компьютере. Попытаться понять, что там за инструкции (не самый лёгкий способ).
Можно пытаться запустить программы для разных архитектур (пока компьютер не взорвётся).
Если компьютер после этих экспериментов больше не включается, можно расковырять его внутренности.
В общем, есть способы.
\\ \\
У меня есть основания предполагать, что архитектура твоего компьютера --- x86.
Как и моего, и вообще, большинства ноутбуков и десктопов, которые ты видишь.
Про эту архитектуру написано много, и я только вкратце упомяну только самое основное.
\\ \\
Программа --- это последовательность команд процессору.
Команды в основном работают с какими-то данными:
они могут принимать на вход \emph{операнды} и возвращать \emph{результат}.
Все эти данные надо где-то хранить.
С точки зрения программы есть три вида памяти:
\begin{enumerate}
\item \textbf{Регистры}
\\
Это маленький кусок сверхбыстрой памяти, зашитой внутри процессора.
Регистры нужны, чтобы не тратить время на доступ к оперативной памяти: загрузка/выгрузка данных выполняется значительно дольше, чем сама команда.
Регистры активно используются для хранения операндов и результатов команд.
Данные в них не задерживаются: они храняться там временно, чтобы вскоре быть использованными и затёртыми новыми данными.
Поскольку регистров мало, у каждого из них есть своё имя, и в командах регистры адресуются всего несколькими битами.
\item \textbf{Оперативная память}
\\
Оперативная память --- это большой кусок медленной памяти, внешней по отношению к процессору.
В оперативной памяти хранится сама программа и все данные, с которыми она работает.
Когда очередной кусок данных нужен для вычислений на процессоре, этот кусок загружается из оперативной памяти в регистры.
Окончательные результаты вычислений выгружаются назад в оперативную память.
Промежуточные результаты, если они не помещаются в регистры, тоже выгружаются в оперативную память.
Оперативная память --- это память с \emph{произвольным доступом} (Random Access Memory, RAM).
Это значит, что одновременно доступны все байты в этой памяти.
Адресом байта служит его порядковый номер.
Поскольку команда может обратиться к любому байту, адрес должен быть целиком зашит в команду.
\item \textbf{Стек}
\\
Стек --- это кусок оперативной памяти со специальными правилами доступа.
Как и оперативная память, стек медленный. Размер стека может быть разным: программа может ограничить себе стек или вообще отказаться от него.
Обычно максимальный размер стека ограничен операционной системой.
Доступ к стеку осуществляется по правилу LIFO (Last In, First Out): одновременно доступно только одно значение (вершина стека).
Адрес вершины стека хранится в специальном регистре, и команды, работающие со стеком, используют этот регистр.
Стек нужен для удобства программистов.
\end{enumerate}
\bigskip
\begin{minipage}{0.6\textwidth}
Физически иерархия памяти устроена более сложно:
между регистровой и оперативной памятью есть ещё несколько уровней \textbf{кэш-памяти}.
В кэш загружаются те данные, которые не попали в регистры, но предположительно скоро понадобятся.
Когда нужны какие-то данные из оперативной памяти, первым делом они ищутся в кэше:
если они там есть (cache hit) --- отлично, если нет (cache miss) --- плохо, придётся тратить время на доступ к памяти.
Программист не управляет кэшами напрямую, как регистрами и оперативной памятью (и может вообще не знать об их существовании).
Есть команды, позволяющие влиять на кэширование данных, но в общем процессор оставляет за собой свободу действий.
\\ \\
Картинка примерно соответствуют семейству процессоров Intel Core
(подробнее о кэшах в мануале ``Intel 64 and IA-32 Architectures Optimization Reference Manual'').
\end{minipage}
\begin{minipage}{0.4\textwidth}
\centering
\includegraphics[width=\textwidth]{pic/caches.png}
\end{minipage}
\bigskip
\\
Регистры не безразмерные: каждый регистр рассчитан на хранение определённого числа битов.
В $N$-битном регистре можно сохранить $2^N$ различных значений.
Это могут быть целые беззнаковые числа в диапазоне $[0, 2^N - 1]$,
или целые числа со знаком в диапазоне $[-2^{N-1}, 2^{N-1} - 1]$,
или адреса байтов в памяти объёмом $2^N$ байт,
или что угодно ещё.
Важно, как понимает эти биты команда, которая работает с регистром.
Размер регистров ограничивает диапазон данных, с которыми работает процессор: данные, которые не влазят в этот диапазон, приходится обрабатывать по кускам.
Число битов в единице данных, с которой работает процессор, определяет его \textbf{разрядность}.
\\ \\
Как и с памятью, физическое устройство регистров может сильно отличаться от логического:
один логический регистр может состоять из нескольких физических или занимать только часть физического.
Некоторые логические регистры вообще не являются регистрами в физическом смысле (скорее набором управляющих сигналов).
В этом смысле разрядность процессора --- понятие сложное и неоднозначное.
Команды работают с логическими регистрами, поэтому нам, как программистам, важно логическое устройство процессора.
\\ \\
Архитектура x86 начиналась с 16-разрядного процессора 8086, сделанного компанией Intel в 1978 году.
Постепенно в неё добавлялись новые возможности: увеличивался размер регистров, появлялись новые регистры, расширялся набор команд.
При этом сохранялось важное свойство: более новые архитектуры x86 были \emph{обратно совместимы} с предыдущими (то есть понимали программы для предыдущих).
Обратная совместимость --- очень полезное свойство: не нужно переписывать старые программы, чтобы запустить их на новом процессоре.
С другой стороны, из-за обратной совместимости приходится таскать за собой много ненужного устаревшего хлама,
что значительно усложняет и замедляет процессор.
У твоего ноутбука 64-разрядная архитектура x86, сокращённо x86-64.
\\ \\
Примерно так выглядят регистры процессора x86-64:
\\
\includegraphics[width=6.5in]{pic/registers.png}
\\
На этой картинке сохранён относительный размер регистров (кроме MSR-ов, размер и число которых зависит от конкретной модели процессора).
Итого есть следующие важные группы регистров:
\begin{itemize}
\item Регистры общего назначения (General Purpose Registers, GPRs).
Это основные регистры, с которыми работает программист:
в них хранятся операнды, результаты команд и вообще всё, что угодно.
Их шестнадцать штук, каждый по 64 бита:
\\
\includegraphics[height=5in]{pic/registers_general_purpose.png}
\\
$i$-й регистр называется $R_i$,
его младшая $32$-битная половина --- $R_iD$ (Double Word).
младшая $16$-битная четверть --- $R_iW$ (Word),
младшая $8$-битная восьмушка --- $R_iB$ (Byte).
Исторически сложилось так, что у первых восьми регистров есть личные имена: RAX, RBX, RCX, RDX, RDI, RSI, RBP и RSP
($32$-битные половины называются EAX, EBX, ECX, EDX, EDI, ESI, EBP и ESP, $16$-битные четверти --- AX, BX, CX, DX, DI, SI, BP и SP,
а $8$-битные восьмушки --- AL, BL, CL, DL, DIL, SIL, BPL и SPL).
Кроме того, старшие байты регистров AX, BX, CX и DX тоже имеют личные имены: AH, BH, CH и DH.
Имена отражают специфику использования каждого из регистров: 'A' --- accumulator, 'B' --- base, 'C' --- counter, 'D' --- data,
'DI' --- destination index, 'SI' --- source index, 'BP' --- base pointer, 'SP' --- stack pointer.
Это не значит, что эти регистры обязательно использовать таким образом, просто некоторые команды придают им специальный смысл.
\item Регистр, хранящий адрес следующей команды (Instruction Pointer, RIP).
Это 64-битный регистр. Хранящийся в нём 64-битный адрес --- это номер байта в оперативной памяти,
с которого начинается следующая инструкция. Процессор исполняет инструкцию и продвигает указатель в регистре RIP.
Программист не может изменять значение регистра RIP напрямую: он должен вызвать специальную инструкцию передачи управления
(\texttt{JMP}, \texttt{Jcc}, \texttt{CALL}, \texttt{RET} и \texttt{IRET}).
Исполняя эту инструкцию, процессор сам передвинет указатель куда надо.
\item Регистр флагов (RFLAGS) --- специальный регистр, содержащий набор флагов. Каждый флаг занимает один-два бита и имеет какой-то специальный смысл.
Старшая половина битов зарезервирована (всегда нулевая). Младшая половина (EFLAGS) выглядит так:
\\
\includegraphics[height=4in]{pic/eflags.png}
\\
\item Сегментные регистры (CS, DS, SS, ES, FS, GS) --- регистры, содержащие адреса \emph{сегментов} памяти.
Физически оперативная память --- это просто массив байтов, и адрес в памяти --- это номер байта.
Логически может быть удобно представлять память как-то по-другому (или вообще использовать разные логические представления в зависимости от случая).
Одно из таких логических представлений --- \emph{сегментная модель памяти}.
В этой модели адресное пространство процесса состоит из кусков --- сегментов.
Разные сегменты соответствуют разным областям физической памяти (которые могут перекрываться и совпадать).
Логический адрес, с которым работает программа, состоит из двух частей: \emph{селектора} и \emph{смещения}.
Селектор отвечает за сегмент, к которому относится адрес, а смещение --- за конкретный байт внутри сегмента.
Логический адрес преобразуется в физический по хитрым правилам.
\\ \\
Сегментная модель позволяет много вариаций в зависимости от числа и взаимного расположения сегментов.
Самая простая модель --- плоская (flat memory model): адресное пространство процесса расположено в одном сегменте.
Плоская модель позволяет забыть о сегментах и работать так, как будто их вообще нет.
Это вырожденный случай сегментирования, но сейчас в основном используется именно плоская модель.
Сегментирование было очень кстати в 16-разрядных процессорах, поскольку в помощью 16-битного адреса можно адресовать всего лишь $2^{16}$ байта
(то есть объём памяти, доступной программе --- всего 64 Кб).
Использование разных сегментов позволило расширить объём доступной памяти.
С переходом на 64-разрядную архитектуру необходимость в разных сегментах отпала: 64 бита позволяют адресовать $2^{64}$ байта,
и пока что используется только часть этих битов.
\\ \\
У процессора есть разные режимы, и в этих режимах используется разное логическое представление памяти.
Основой режим --- защищённый (protected mode) --- требует использования сегментной модели памяти.
Другое дело, что обычно все сегменты слиты в один, поэтому сегментирование незаметно.
Подробнее о режимах процессора и моделях памяти читай в мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 3, глава 2.
\item Регистры для работы с числами с плавающей запятой (Floating Point Registers).
Это восемь 80-битных регистров, организованных в виде стека (ST0 - ST7).
К ним добавляются контрольные регистры: 16-битные CW (Control Word), SW (Status Word) и TW (Tag Word),
11-битный FP\_OPC (Opcode) и 64-битные FP\_IP (Instruction Pointer) и FP\_DP (Data Pointer).
Всё вместе устройство называется \emph{сопроцессором FPU} (Floating Point Unit), или расширением x87.
Подробнее о работе с FPU читай в мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 1, глава 8.
\item Регистры для работы с векторными инструкциями (Scalar Instruction Multiple Data, SIMD).
Это регистры для проведения векторных операций: например, для одновременного сложения не одной, а четырёх пар чисел.
К ним относятся регистры MMX, физически совмещённые с регистрами FPU; регистры SSE, AVX и т.д.,
а также контрольный регистр MXCSR (Control/Status Register).
Подробнее о векторных инструкциях и расширениях для работы с ними читай в мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 1, главы 9 - 14.
\item Системные регистры.
Это регистры, с которыми возятся в основном разработчики операционной системы.
Большинство из этих регистров с горем пополам описаны в мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 3
(так что тебе придётся прочитать его, когда надумаешь писать операционную систему для x86).
Вот основные группы системных регистров:
    \begin{itemize}
    \item Контрольные регистры (Control Registers) CR0 - CR4, CR8, XCR0.
    Они определяют режим работы процессора и характеристики текущей исполняемой задачи.
    \item Регистры управления памятью (Descriptor Table Registers) GDTR, IDTR, LDTR, TR.
    Регистр IDTR хранит указатель на таблицу дескрипторов обработчиков прерываний IDT (Interrupt Descriptor Table).
    Регистр TR хранит информацию о текущей выполняемой задаче.
    Регистры GDTR и LDTR нужны при использовании сегментной модели памяти:
    они хранят указатели на таблицы сегментных дескрипторов GDT (Global Descriptor Table) и LDT (Local Descriptor Table).
    Эти таблицы участвуют в процессе преобразования логического адреса в физический:
    \\
    \includegraphics[height=3.5in]{pic/memory_management.png}
    \\
        \begin{enumerate}
        \item Логический адрес преобразуется в \emph{линейный} адрес.
        Процессор использует первую часть логического адреса --- селектор --- для чтения сегментного дескриптора из таблицы.
        Сегментный дескриптор содержит адрес начала сегмента в памяти (\emph{базовый} адрес) и атрибуты сегмента (размер, права доступа и т.д.).
        Процессор проверяет, попадает ли смещение (вторая половина логического адреса) внутрь сегмента и не нарушены ли права доступа к сегменту.
        Сумма базового адреса и смещения даёт линейный адрес.
        \item Линейный адрес преобразуется в физический адрес.
        Этот этап зависит от того, используется ли \emph{страничный механизм} управления памятью (paging).
        Страничный механизм --- это ещё одно логическое представление поверх сегментной модели памяти.
        Оно нужно для того, чтобы создавать у программ иллюзию очень большого адресного пространства --- виртуальной памяти.
        Виртуальная память разбита на куски (станицы), и в физической памяти присустствует только часть этих страниц
        (те, которые нужны программе прямо сейчас).
        Если программе понадобится страница, отсутствующая в физической памяти, придётся подгрузить её.
        Это займёт некоторое время и, возможно, приведёт к вытеснению других страниц, но в общем ничего страшного не случится
        (это событие называется ``page fault'').
        Сама программа ничего не знает о страницах и page fault'ах --- управлением занимается операционная система.
        Страничный механизм похож на кэш процессора, а page fault похож на cache miss:
        если данных нет в кэше, их приходится подгружать из оперативной памяти;
        если данных нет в оперативной памяти, их приходится подгружать с внешней памяти.
        \end{enumerate}
    Страничный механизм использовать не обязательно, но обычно он используется в мультизадачных операционных системах.
    В линуксе есть программа \texttt{top} и её более красивый вариант \texttt{htop} --- они показывают текущие выполняемые задачи.
    Там хорошо видна разница между потреблением виртуальной и физической памяти (колонки \texttt{VIRT} и \texttt{RES}):
    Программа может есть очень много виртуальной памяти и сравнительно немного физической.
    \\ \\
    Подробнее об управлении памятью в защищённом режиме читай в мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 3, главы 3 - 4.
    \item Регистры для дебага (Debug Registers) DR0 - DR15.
    Подробнее в мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 3, глава 17.
    \item Регистры, зависящие от конкретной модели процессора (Model-Specific Registers, MSRs).
    Это огромное множество регистров для самых разных нужд.
    Часть из них присутствует во всех новых моделях --- это подмножество называется Architectural MSRs.
    С большего MSR'ы описаны в мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 3, глава 35.
    Упомяну наиболее интересные группы MSR'ов:
        \begin{itemize}
        \item Регистры для управления кэшированием данных (Memory Type Range Registers, MTRRs).
        Есть разные стратегии кэширования: они определяют, кэшировать ли данные в принципе,
        когда записывать данные в кэш, когда записывать данные из кэша в оперативную память,
        когда синхронизировать кэши между собой и т.д.
        Некотороые области памяти вообще нельзя кэшировать (например, область памяти, через которую какое-то устройство общается с процессором (memory-mapped I/O)).
        Иногда при записи данных из кэша в память можно не соблюдать порядок изменений
        (например, при записи в память видеокарты: не важно, в каком порядке меняются отдельные пиксели --- главное, чтобы картинка на экране менялась).
        Иногда надо синхронизировать кэш с памятью каждый раз, когда данные в кэше меняются.
        Регистры MTRR позволяют разным областям оперативной памяти сопоставить разные стратегии кэширования.
        Подробнее об управлении кэшами в мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 3, глава 11.
        \item Регистры для контроля аппаратных ошибок (Machine Check Registers).
        Подробнее об обнаружении и исправлении аппаратных ошибок в мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 3, глава 15.
        \item Регистры для замера производительности (Performance Counters).
        Это счётчики всяких событий типа
        процессорного времени, затраченного на задачу (task-clock)
        тактов процессора (cycles),
        переключений контекста на процессоре (context switch),
        неудачный предсказаний условных переходов (branch miss),
        отсутствия нужных данных в кэше (cache miss),
        отсутствия нужной страницы в памяти (page fault)
        и т.д.
        Аппаратная поддержка замера производительности --- большое дело, она позволяет в точности выяснить,
        где и почему программа тормозит и как это исправить. Иногда достаточно чуть-чуть поменять порядок вычислений,
        чтобы получить большой прирост производительности.
        \end{itemize}
    \end{itemize}
\end{itemize}
Архитектуры процессоров меняются со свистом, поэтому уже через несколько лет всё будет слегка (или сильно) по-другому.
Я опиралась на последнюю версию интеловского мануала (он лежит на сайте и время от времени обновляется).
В мануале описание скорее всего не точное, наверняка неполное и скоро устареет.
Важно примерно представлять себе архитектуру.
\\ \\
Программы, сгенерированные нашим компилятором, будут использовать память, стек и несколько регистров общего назначения (их младшие 32 бита).
В нашем языке размер чисел не ограничен --- это просто любые целые числа.
В компиляторе мы ограничимся 32-битными целыми числами со знаком (то есть числами в диапазоне $[- 2^{31}, 2^{31} - 1]$).
Я выбрала 32, а не 64 бита потому, что это упростит кодирование команд.
Кстати, в парсере и интерпретаторе тоже зашито ограничение на 32 бита --- для хранения чисел используется тип \texttt{int}.
\\ \\
Теперь, когда ясно, \emph{с чем} работают команды, пора поговорить собственно о командах: какие они бывают и как кодируются.
Понятно, что без некоторых команд не обойтись.
Необходимый минимум включает команды пересылки данных между памятью и регистрами,
арифметико-логические команды
и команды передачи управления (как минимум условного перехода).
Можно на этом остановиться --- ограничиться только самыми простыми командами (и некоторые архитектуры так и делают).
А можно добавить ещё много полезных (или не совсем) команд.
Как бы там ни было, команды неравноценны по времени выполнения:
команды, работающие с памятью, выполняются здорово дольше других (на кэш можно надеяться, но не стоит рассчитывать).
От этой неравноценности никуда не денешься, и она порождает два принципиальных подхода к составлению набора команд:
\textbf{CISC} (который только усиливает неравноценность) и \textbf{RISC} (который пытается сгладить неравноценность).
\\ \\
Подход CISC (Complex Instruction Set Computer) возник из-за неудобства программирования на ассемблере:
очень уж большой разрыв между высокоуровневыми идеями в человеческой голве и низкоуровневыми машинными инструкциями.
На каждый чих приходится писать по маленькой программе.
Часто приходится повторять одни и те же рутинные последовательности действий, одни и те же цепочки команд.
Естественная мысль --- заменить их одной сложной командой.
Понятно, что такие сложные команды приведут к ещё большему расслоению команд по времени выполнения,
особенно если появится много команд, работающих с памятью.
Зато сложные команды можно оптимизировать на уровне микросхем.
\\ \\
Второй подход --- RISC (Reduced Instruction Set Computer) --- возник, когда стало ясно, что часто ассемблерная программа из простых команд
более эффективна, чем сложная команда. Между простыми командами легче отслеживать \emph{зависимости},
поэтому часто их удаётся переставлять местами и выполнять параллельно.
Основная идея RISC-архитектуры --- сделать так, чтобы почти все команды выполнялись быстро (за один или несколько тактов процессора).
Поскольку доступ к памяти занимает много времени, оставляют только две команды работы с памятью: загузка и выгрузка.
\\ \\
CISC-архитектуру можно реализовать по-разному.
Первый, очевидный, способ --- усложнять процессор, добавляя в него всё новые и новые куски микросхем.
Этот способ имеет два недостатка: процессор становится очень сложным, а набор команд --- узкоспециализированным или очень большим.
Поэтому обычно идут другим путём: делают препроцессор, который раскладывает сложные команды на более простые.
Этот препроцессор называется \emph{интерпретатором микрокода}, а сами сложные команды --- \emph{микропрограммами}.
Микропрограммы хранятся в специальной памяти.
Чтобы добавить или поменять команду, нужно просто сохранить её микрокод в эту память.
При таком подходе внутренний процессор остаётся простым (причём это вполне может быть RISC-, а не CISC-процессор),
а в выборе команд появляется невероятная гибкость: теоретически кто угодно может составить удобный для себя набор команд.
На практике далеко не все производители процессоров открывают доступ к микрокоду.
\\ \\
Неправильно думать, что CISC-архитектуры сложные, а RISC-архитектуры простые.
Слова ``complex'' и ``reduced'' относятся к скорости выполения команд, а не к их многообразию.
Есть очень простые CISC-ахитектуры и очень сложные RISC-архитектуры.
Многие архитектуры смешивают в себе RISC и CISC черты.
Об этом всём хорошо написано в статье ``Processor Architectures. RISC-CISC-Harvard-Von Neumann''. :)
\\ \\
x86 --- пример очень сложной CISC-архитектуры.
Её сложность ты можешь на глаз оценить по размеру мануала ``Intel 64 and IA-32 Architectures Software Developer's Manual''.
\\ \\
Команды в x86 кодируются байтовыми последовательностями разной длины (от 1 до 15 байт).
Каждя команда начинается с \emph{опкода}, который занимает от одного до трёх байт.
У каждой команды свой, уникальный опкод (поэтому её нельзя спутать с другой командой).
После опкода могут быть другие, командо-специфичные байты. В них закодированиы
операнды: номера регистров, адреса в памяти или \emph{непосредственные операнды}
(т.е. константы, зашитые прямо в инструкцию). Детальное описание всех команд x86
ты найдёшь в мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 2.
\\ \\
Ну что ж, немного обсудили целевую архитектуру --- пора переходить к генерации кода.
Чтобы понять, как компилятор генерирует машинный код, возьмём какую-нибудь программу и скомпилируем её вручную.
Для начала возьмём совсем простую программу: $1 + 2$.
Команда для сложения --- \texttt{ADD}.
Откроем мануал ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 2, на описании команды \texttt{ADD}:
\\ \\
\includegraphics[width=6.5in]{pic/21a.png}
\\
\includegraphics[width=6.5in]{pic/21b.png}
\\
\includegraphics[width=6.5in]{pic/21c.png}
\\ \\
О-хо-хо! На одно только описание команды \texttt{ADD} ушла целая страница.
Что мы здесь видим?
\\ \\
Команда \texttt{ADD} описывается таблицей. Строки таблицы --- это разные варианты команды.
Колонки отвечают за разные характеристики команды:
``Opcode'' --- схема кодирования команды,
``Instruction'' --- общий вид команды,
``Op/En'' (Operand Encoding) --- тип кодирования операндов,
``64 Bit Mode'' и ``Compat/Leg Mode'' (Compatibility/Legacy Mode) --- работает ли команда в этих режимах процессора
(если помнишь, у процессоров x86 есть разные режимы),
``Description'' --- описание команды на человеческом языке.
\\ \\
У команды \texttt{ADD} есть четыре основных варианта, различающихся по типу операндов.
Это хорошо видно в колонке ``Op/En'': там встречаются четыре разных значения
I (immediate), MI (register/memory - immediate), RM (register - register/memory), MR (register/memory - register).
Каждый из этих четырёх вариантов дальше разветвляется по размеру операндов.
\\ \\
Какой вариант подходит нам?
В нашей программе числа 1 и 2 --- это константы (immediate operand).
Мы договорились, что компилятор работает с 32-битными числами, поэтому это 32-битные константы.
(Из таблицы должно быть понятнее, почему я выбрала 32, а не 64 бита: для 64-битных команд впереди добавляется некий REX-префикс, с которым возиться неохота.)
Нам подошла бы команда \texttt{ADD imm32, imm32}, но такой нет.
Зато есть очень похожие команды \texttt{ADD EAX, imm32} и \texttt{ADD r/m32, imm32}.
Команда \texttt{ADD EAX, imm32} берёт первое слагаемое из регистра EAX.
Команда \texttt{ADD r/m32, imm32} берёт первое слагаемое из любого регистра общего назначения или из памяти.
Понятно, что обе команды не идеальны: всё равно придётся загрузить первое слагаемое в память или в регистр.
Для этого нам понадобится другая команда --- \texttt{MOV}:
\\ \\
\includegraphics[width=6.5in]{pic/mov_1.png}
\\
\includegraphics[width=6.5in]{pic/mov_2.png}
\\
\includegraphics[width=6.5in]{pic/mov_3.png}
\\
\includegraphics[width=6.5in]{pic/mov_4.png}
\\
\includegraphics[width=6.5in]{pic/mov_5.png}
\\
\includegraphics[width=6.5in]{pic/mov_6.png}
\\
\includegraphics[width=6.5in]{pic/mov_7.png}
\\ \\
Час от часу не легче: вариантов ещё больше, чем у команды \texttt{ADD}.
И это только \texttt{MOV} для регистров общего назначения, а есть ещё \texttt{MOV}ы для другигих групп регистров.
Режимов кодирования операндов тоже прибавилось: кроме уже известных MI, RM и MR появились ещё FD, TD и OI.
Нам подходят варианты \texttt{MOV r32, imm32} и \texttt{MOV r/m32, imm32}.
\\ \\
Пора определиться: в регистр или в память мы будем записывать первое слагаемое,
и если в регистр, то в EAX или нет (команда \texttt{ADD} предусматривает отдельный вариант для EAX).
В архитектуре x86 часто возникают такие ситуации, когда одно и то же можно сделать несколькими разными способами.
Хороший компилятор старается всегда выбирать самый быстрый вариант.
В нашем случае, во-первых, регистры быстрее памяти --- поэтму следует хранить первое слагаемое в регистре.
Во-вторых, команда \texttt{ADD} явно оптимизирована для регистра EAX, поэтому следует хранить первое слагаемое в регистре EAX.
С учётом этого у нас получается такая ассемблерная программа:
\begin{verbatim}
    mov eax, 1
    add eax, 2
\end{verbatim}
Неплохо! Осталось перевести эту программу в машинные коды, то есть ассемблировать.
Как это сделать?
Самый правильный (но не самый простой) способ --- прибегнуть к старому другу, интеловскому мануалу.
Там подробно объясняется кодирование всех команд.
Я разберу только две команды из нашего примера --- полную схему кодирования ты всегда найдёшь в мануале.
\\ \\
Начнём с команды \texttt{mov eax, 1}.
В таблице ей соответствует вариант:
\\
\includegraphics[width=6.5in]{pic/mov_r32_imm32.png}
\\
В первой колонке указана схема кодирования: \texttt{B8 + rd id}.
В мануале ``Intel 64 and IA-32 Architectures Software Developer's Manual'', том 2, глава 3.1.1, поясняются эти обозначения:
\begin{itemize}
\item \texttt{B8} --- однобайтный шестнадцатеричный опкод.
\item \texttt{rd} (register dword) --- однобайтный номер регистра.
Регистру EAX соответствует номер 0, поэтому байт равен \texttt{0x00}.
\item \texttt{B8 + rd} означает, что эти два байта надо сложить: \texttt{0xB8 + 0x00 = 0xB8}.
\item \texttt{id} (immediate dword) --- это 4-байтный непосредственный операнд.
У нас непосредственный операнд --- это константа 1, в шестнадцатеричном представлении --- \texttt{0x1}.
На её кодирование отводится целых четыре байта, в то время как сама константа занимает один бит.
Все остальные биты придётся забить нулями: \texttt{0x00 0x00 0x00 0x01}.
Это кажется напрасной тратой битов, но ведь вместо 1 могло бы быть любое число в диапазоне $[-2^{N - 1}, 2^{N - 1} - 1]$.
\end{itemize}
Итого, получаем команду --- \texttt{0xB8 0x00 0x00 0x00 0x01}, так?
\\ \\
Так, да не совсем.
Есть одна гадкая мелочь: почему мы решили, что 4-байтное число 1 должно кодироваться именно как \texttt{0x00 0x00 0x00 0x01}?
Потому что это \emph{естественно}: мы привыкли представлять числа от старших разрядов к младшим.
Однако число ничуть не изменится, если его представлять как-то по-другому: например, от младших разрядов к старшим.
Это дело договорённости: важно, чтобы все понимали представление одинаково.
Так вот архитектура x86 представляет себе числа от младших \emph{байтов} к старшим:
например, 4-байтное число \texttt{0x0A0B0C0D} предсталено в виде \texttt{0x0D 0x0C 0x0B 0x0A}.
Такой порядок байт называется \emph{little-endian}.
Другой порядок --- \emph{big-endian}, в нём байты хранятся от старших к младшим: \texttt{0x0A}, \texttt{0x0B}, \texttt{0x0C}, \texttt{0x0D}.
Само понятие порядка байт называется \textbf{endianness}.
\\
\includegraphics[height=1.75in]{pic/22.png}
\\
Исправленная команда выглядит так: \texttt{0xB8 0x01 0x00 0x00 0x00}.
\\
\includegraphics[height=1.25in]{pic/mov_eax_1.png}
\\ \\
Теперь ассемблируем вторую команду: \texttt{add eax, 2}.
Мы решили использовать вариант:
\\
\includegraphics[width=6.5in]{pic/add_eax_imm32.png}
\\
На сей раз схема кодирования совсем простая: \texttt{05 id}.
Понятное дело, \texttt{05} --- однобайтный шестнадцатеричный опкод,
а \texttt{id} --- 4-байтный непосредственный операнд в форме little-endian.
Итого получаем команду \texttt{0x05 0x02 0x00 0x00 0x00}:
\\
\includegraphics[height=1.25in]{pic/add_eax_2.png}
\\ \\
Объединяя обе команды, получаем итоговую программу: \texttt{0xB8 0x01 0x00 0x00 0x00 0x05 0x02 0x00 0x00 0x00}.
\\ \\
На самом деле можно было получить её гораздо проще с помощью какого-нибудь готового ассемблера.
(Здесь я понимаю ассемблер как программу, а не как язык.)
Синтаксис разных ассемблеров немного отличается.
Для ассемблера \texttt{nasm} исходная программа выглядит так (файл example.asm):
\lstinputlisting{nasm/example.asm}
Пришлось добавить директиву \texttt{BITS 64} --- она указывает ассемблеру, для какого режима процессора ассемблировать программу.
Ассемблируем:
\begin{verbatim}
$ nasm -O0 example.asm
\end{verbatim}
Флаг -O0 (нулевые оптимизации) заставляет \texttt{nasm} сохранять исходные команды
(по умолчанию \texttt{nasm} подыскивает более короткие эквивалентные команды).
Содежимое бинарного файла example можно посмотреть с помощью \texttt{mc}, открыв файл по \texttt{F3} и затем нажав \texttt{F4}:
\begin{verbatim}
0xB8 0x01 0x00 0x00 0x00 0x05 0x02 0x00 0x00 0x00
\end{verbatim}
Это полностью совпадает с тем, что мы наассемблировали руками.
Теперь можно из любопытства опустить флаг -O0:
\begin{verbatim}
$ nasm example.asm
\end{verbatim}
Первая команда (\texttt{mov eax, 1}) осталась неизменной, а вот вторую команду (\texttt{add eax, 2})
\texttt{nasm} оптимизировал: он заменил её более короткой командой \texttt{add r/m32, imm8}
(воспользовавшись тем, что константа 2 влазит в один байт):
\begin{verbatim}
0xB8 0x01 0x00 0x00 0x00 0x83 0xC0 0x02
\end{verbatim}
(Команда \texttt{add r/m32, imm8} узнаётся по опкоду \texttt{0x83} из таблицы для команды \texttt{ADD}).
Вот так мы узнали, что оказывается можно было составить более эффективную программу.
\\ \\
Дальше я не буду останавливаться на кодировании команд.
Некоторые команды кодируются намного хитрее, чем те, что мы видели,
особенно команды, работающие с памятью: у них есть сложные режимы адресации.
В нашем компиляторе будут использоваться в основном простые команды.
Это не всегда будут самые эффективные команды, но я и не замахивалась на оптимизирующий компилятор ---
понятно, что исходный язык настолько прост, что позволяет любую программу вычислить на этапе компиляции
(программы не зависят ни от каких внешних данных).
\\ \\
Ну что ж, побаловались с маленькой программой, пора переходить к компиляции принципиально \emph{любой} программы.
Чтобы не погибнуть в мутных примерах и частных случаях, понадобятся радикальные меры. :D
\\ \\
Как говорится, ``воспользуемся методом математической индукции''.
Программа представлена в виде AST, и индукцию мы будем проводить по высоте этого AST.
Листьями AST являются числа, узлами --- арифметические операции.
Сначала мы рассмотрим AST нулевой высоты: лист, и покажем, как его скомпилировать --- это будет база индукции.
Затем мы предположим, что любой AST высоты не более $k$ скомпилирован, и покажем, как на его основе скомпилировать любой AST высоты $k + 1$ --- шаг индукции.
Поехали!
\\ \\
Итак, база индукции: нужно скомпилировать лист AST, то есть константу $C$.
Нет ничего проще:
\begin{verbatim}
    mov eax, С
\end{verbatim}
Переходим к шагу индукции: нужно скомпилировать программу $p_1 \circ p_2$,
где $p_1$ и $p_2$ --- уже скомпилированные программы, а $\circ$ --- арифметическая операция.
Сразу возникает вопрос: где хранятся результаты выполнения программ $p_1$ и $p_2$?
Если $p_1$ --- лист, то результат $p_1$ хранится в регистре EAX.
Однако если $p_2$ --- тоже лист, то результат $p_2$ хранится тоже в регистре EAX!
Очевидно, если $p_2$ выполняется после $p_1$, то $p_2$ затрёт результат $p_1$:
\begin{verbatim}
    <p1>          ; result in EAX
    <p2>          ; result in EAX
    <op> eax, eax ; bad :(
\end{verbatim}
Что с этим делать?
Можно попытаться заставить разные подпрограммы использовать разные регистры:
например, пусть $p_1$ сохраняет результат в EAX, а $p_2$ --- в EBX:
\begin{verbatim}
    <p1>          ; result in EAX
    <p2>          ; result in EBX
    <op> eax, ebx ; result in EAX
\end{verbatim}
Всё хорошо, пока не задумываешься, что из себя представляют $p_1$ и $p_2$.
Это ведь \emph{любые} программы, они могут быть очень большими и ветвиться на сколько угодно подпрограмм.
Все эти подпрограммы как-то делят между собой регистры.
Что если подпрограмм больше, чем регистров?
Например, если $p_1 = C_1 + (C_2 + (... + (C_{N - 1} + C_N)...))$, то по нашей логике программа для $p_1$ должна скомпилироваться так:
\begin{verbatim}
    mov r1, C1         ; result in r1
    mov r2, C2         ; result in r2
    ...
    mov r(N-1), C(N-1) ; result in r(N-1)
    mov r(N), C(N)     ; result in r(N)
    add r(N-1), rN     ; result in r(N-1)
    ...
    add r1, r2         ; result in r1
\end{verbatim}
Ясно, что для больших $N$ регистров не хватит.
Есть и другая неприятность: запрет использования отдельных регистров вносит асимметрию.
Две одинаковых подпрограммы могут скомпилироваться по-разному только потому, что им будут доступны разные регистры.
Сама компиляция тоже усложняется: нужно постоянно следить за тем, какие регистры свободны.
Короче, ничё хорошего.
Явно регистрами тут не обойтись.
\\ \\
Очевидный выход --- сохранять результат программы $p_1$ в память.
(Результат программы $p_2$ сохранять не надо, он сразу же используется.)
На вопрос ``а если памяти не хватит?'' я могу ответить только ``будет плохо!''.
На самом деле, если нам попадётся \emph{настолько} большая программа, проблемы начнутся гораздо раньше
и решать их придётся другими способами (разбивать программу на куски).
Нас эти проблемы не особо интересуют, поскольку они общие для большинства программ, а не специфичные для компилятора.
Будем считать, что памяти много.
\\ \\
Итак, сохраняем результат программы $p_1$ в память (допустим, у нас есть большой кусок памяти):
\begin{verbatim}
    <p1>
    mov [addr], eax ; result in [addr]
    <p2>            ; result in eax
    mov ebx, [addr]
    <op> eax, ebx   ; result in eax
\end{verbatim}
Эта схема вполне работоспособна и можно было бы на ней остановиться.
Но она, честно говоря, малось корявая.
Компилятору придётся много возиться с адресами:
программа $p_2$ может рекурсивно ветвиться на другие подпрограммы, которые тоже будут сохранять результат в память.
Компилятору придётся запоминать, где чей результат.
Хуже того: когда результат подпрограммы читается из памяти, он больше не нужен, и хорошо бы эту память использовать повторно.
Значит, компилятору придётся следить за тем, какие ячейки памяти свободны (ну или использовать память нерационально).
\\ \\
Этого всего можно избежать, если заметить, что в нашем случае порядок вычислений позволяет хранить значения в стеке.
\\
\bigskip
\begin{minipage}{0.6\textwidth}
Стек --- это кусок памяти, доступ к которому осуществляется с помощью команд \texttt{PUSH} и \texttt{POP}:
\texttt{PUSH} ``запихивает'' значение на вершину стека, а \texttt{POP} ``выпихивает'' последнее записанное значение.
Получается, что значения читаются в порядке, обратном порядку записи.
Указатель на вершину стека храниться в регистре ESP (Stack Pointer), и команды \texttt{PUSH} и \texttt{POP} сами обновляют его:
\texttt{PUSH} уменьшает ESP на размер операнда и записывает операнд в память по адресу [ESP],
а \texttt{POP} читает значение из памяти по адресу [ESP] в операнд и увеличивает ESP на размер операнда.
\end{minipage}
\begin{minipage}{0.4\textwidth}
\centering
\includegraphics[width=\textwidth]{pic/stack.png}
\end{minipage}
\bigskip
\\
Казалось бы, довольно специфичная структура --- но стек оказывается полезен во многих задачах.
Он позволяет работать со вложенными (древовидными) структурами:
узел-потомок начинает обрабатывться позже, чем узел-предок, а заканчивает раньше
(что соответствует принципу LIFO).
Если бы AST представляло из себя не дерево, а произвольный граф, то стеком мы бы не отделались.
\\ \\
С учётом стека произвольный узел AST вида $p_1 \circ p_2$ компилируется так:
\begin{verbatim}
    <p1>
    push eax
    <p2>
    pop ebx
    <op> eax, ebx ; result in eax
\end{verbatim}
Это уже очень хороший и почти окончательный вариант. :)
Осталось подправить две мелочи, связанные с кодированием команд.
\\ \\
Первая мелочь касается неразберихи с битностью, командами и режимами процессора.
Мы нигде явно не договаривались, для какого режима процессора мы компилируем программы.
Помнится, я сказала, что мы будем использовать 32-битную, а не 64-битную арифметику, потому что она проще кодируется.
Значит ли это, что мы автоматически выбрали 32-битный режим?
Нет.
Команды 32-битной арифметики доступны в обоих режимах (это написано в их таблицах в колонках ``64 Bit Mode'' и ``Compat/Leg Mode'').
Давай определимся: пусть наши программы будут 64-битные --- как-никак мы компилируем для 64-битного процессора.
Это решение никак не повлияет на арифметические команды, но немного повлияет на команды \texttt{PUSH} и \texttt{POP}.
Как видно из таблицы для команды \texttt{PUSH}, в 64-битном режиме нет команды \texttt{PUSH r/m32} --- есть только команда \texttt{PUSH r/m64}.
Эти две команды кодируются абсолютно одинаково --- строго говоря, это одна команда, интерпретация которой зависит от режима.
Аналогично с командой \texttt{POP}.
Исправленная программа выглядит так:
\begin{verbatim}
    <p1>
    push rax
    <p2>
    pop rbx
    <op> eax, ebx ; result in eax
\end{verbatim}
Может показаться, что нельзя так перемешивать 32-битные и 64-битные регистры,
но заметь, что в вычислениях участвуют только 32-битные регистры, а их старшие половины просто занимают лишнее место в стеке.
\\ \\
Вторая мелочь связана с кодированием арифметических команд.
У нас первый операнд находится в регистре EBX, а второй --- в регистре EAX.
Результат мы хотим получить в регистре EAX.
Со сложением всё хорошо: есть команда \texttt{add eax, ebx}, котоая прибавляет EBX к EAX.
С умножением тоже всё хорошо: команда \texttt{imul ebx} умножает EAX на EBX.
Из-за коммутативности сложения и умножения нам безразличен порядок операндов.
С вычитанием всё хуже: команда \texttt{sub eax, ebx} вычитает EВX из EAX (перепутаны уменьшаемое и вычитаемое),
а команда \texttt{sub ebx, eax} делает что надо, но записывает результат в EBX ---
значит, придётся менять операнды местами или перемещать результат.
С делением совсем плохо: делимое обязательно должно быть в регистре EAX (других вариантов просто нет).
Очевидное решение --- поменять операнды местами (для этого есть даже специальная команда \texttt{XCHG}).
Но есть более элегантное решение: можно поменять местами подпрограммы $p_1$ и $p_2$:
\begin{verbatim}
    <p2>
    push rax
    <p1>
    pop rbx
    <op> eax, ebx ; result in eax
\end{verbatim}
Наконец, есть ещё одна мелочь:
произведение двух 32-битных чисел может быть 64-битным числом, поэтому команда \texttt{IMUL}
записывает результат не просто в EAX, а в пару регситров EDX:EAX.
Мы работаем с 32-битными числами и игнорируем старшую часть результата: что не влезло, то не влезло.
Но с командой \texttt{IDIV} просто проигнорировать не получится: она берёт старшую часть делимого из регистра EDX,
поэтому если там будет мусор, результат деления будет неправильным.
Перед делением мы должны заполнить регистр EDX старшим (знаковым) битом делимого (sign-extend EDX of EAX): если делимое положительное, EDX = 0,
если отрицательное --- EDX = 0xFFFFffff.
В этом нам поможет команда \texttt{CDQ} (Convert Double to Quad).
(Если непонятно, почитай про двоичное представление отрицательных чисел в виде дополнения до двух: two's complement.)
\\ \\
Окончательный алгоритм компиляции AST в код для x86-64 выглядит так:
\begin{verbatim}
compile(p):
    if p = C:
        mov eax, C
    else if p = p1 + p2:
        compile(p2)
        push rax
        compile(p1)
        pop rbx
        add eax, ebx
    else if p = p1 - p2:
        compile(p2)
        push rax
        compile(p1)
        pop rbx
        sub eax, ebx
    else if p = p1 * p2:
        compile(p2)
        push rax
        compile(p1)
        pop rbx
        imul ebx
    else if p = p1 / p2:
        compile(p2)
        push rax
        compile(p1)
        pop rbx
        cdq
        idiv ebx
\end{verbatim}
Всё!
С генерацией кода разобрались. :)
Вот кусок компилятора, который генерирует код по AST:
\lstinputlisting[language=C++]{example_lang_proc/compiler.cpp}
Компилятор, как и интерпретатор, рекурсивно обходит AST,
только вместо немедленных вычислений он генерирует машинные инструкции и сохраняют их в массив байтов.
Компиляцию операндов я вынесла в отдельную функцию, потому что она везде повторяется.
\\ \\
Вообще говоря, на этом задача компилятора выполнена: мы разобрались с архитектурой x86 и научлились компилировать AST в машинный код.
Но для души этого мало: надо ещё запустить скомпилированную непосильным тудом программу на настоящем процессоре.
(Как минимум, проверить, что она работает. :D)
\\ \\
Обычно компилятор сохраняет скомпилированную программу в виде объектного файла.
Объектный файл содержит машинный код и разную информацию: статические данные,
таблицу симовлов (экспортируемые и импортируемые функции, глобальные переменные и т.д.),
релокации (адреса в программе надо настраивать в соответствии с адресом, по которому программа будет загружена в память),
информацию для дебага и прочее.
Когда компилятор скомпилировал все единицы трансляции в объектные файлы, наступает черёд линкера.
Линкер связывает все объектные файлы в один исполняемый файл: настраивает релокации, разрешает зависимости и т.д.
Линковка --- отдельная тема, сложная и интересная, и я не буду её трогать.
Нам линкер не нужен --- наши программы всегда состоят из одной самодостаточной единицы трансляции и не возятся с адресами в памяти.
Мы можем сразу же сохранять программу в виде исполняемого файла.
\\ \\
Формат исполняемых файлов специфичен для операционной системы.
В линуксе принят формат ELF (Executable and Linkable Format), и мы рассмотрим его 64-битную версию: ELF-64.
Описание этого формата ты можешь найти в \texttt{man elf} (или в интернедах).
\\ \\
\begin{minipage}{0.5\textwidth}
ELF-файл предназначен для хранения двух типов файлов: объектных (relocatable) и исполняемых (loadable).
С объектными файлами работает линкер, с исполняемыми --- загрузчик.
\\ \\
Любой ELF-файл начинается с ELF-заголовка, который
описывает ключевые характеристики файла и взаимное расположение остальных частей файла:
таблицы программных заголовков, таблицы разделов и самих сегментов или разделов.
Таблица программных заголовков необязательна для объектных файлов, а таблица разделов необязательна для исполняемых файлов.
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{pic/elf.png}
\end{minipage}
\\ \\
\begin{minipage}{0.75\textwidth}
Наши ELF-файлы будут состоять из трёх частей:
\begin{enumerate}
\item ELF-заголовок.
\item Таблица программных заголовков с единственным заголовком, описывающим сегмент кода.
\item Сегмент кода, содержащий код программы.
\end{enumerate}
\end{minipage}
\begin{minipage}{0.25\textwidth}
\centering
\includegraphics[width=\textwidth]{pic/elf2.png}
\end{minipage}
\\ \\
Как нам проще всего сгенерировать заголовки?
Можно руками записать нужные байты в файл, пользуясь спецификацией и внимательно следя за размером и содержимым каждого поля.
Но в линуксе есть С-шный заголовочный файл ``elf.h'', в котором определены структуры заголовков и много удобных макросов и констант с красивыми названиями.
Использование этого хедера сделает наш компилятор непортабельным:
компилятор невозможно будет собрать в другой операционной системе, где нет такого хедера.
Но честно говоря, наш компилятор и так не отличается особой портабельностью:
генерирует код для специфичной архитектуры x86\_64 и сохраняет его в специфичном формате ELF-64.
\\ \\
ELF-заголовок состоит из следующих полей:
\\ \\
\begin{minipage}{0.5\textwidth}
\begin{itemize}
\item Группа полей ``ELF Identification'':
    \begin{itemize}
    \item Четырёхбайтовая сигнатура \texttt{0x7f 0x45 0x4c 0x46} (``magic'').
    Если ты откроешь в \texttt{mc} по Shift+F3 какой-нибудь ELF-файл, ты увидишь эту сигнатуру: первый символ (\texttt{0x7f}) непечатный, а остальные три
    (\texttt{0x45 0x4c 0x46}) образуют слово ``ELF''.
    Наличие уникальной сигнатуры характерно для бинарных форматов.
    \item Класс (разрядность): у нас ELFCLASS64.
    \item Кодирование данных: у нас ELFDATA2LSB.
    \item Номер версии формата ELF:у нас EV\_CURRENT.
    \item OS ABI (Operation System Application Binary Interface, бинарный интерфейс операционной системы): у нас, видимо, ELFOSABI\_LINUX.
    \item Номер версии OS ABI: в \texttt{man elf} советуют 0.
    \item Семь зарезервированных байтов (нулевых).
    \end{itemize}
\item Тип: у нас ET\_EXEC (исполняемый).
\item Архитектура: у нас EM\_X86\_64.
\item Номер версии файла: у нас, видимо, EV\_CURRENT.
\item Точка входа (адрес в памяти, на который загрузчик передаст управление): пока непонятно, откуда его взять.
\item Смещение таблицы программных заголовков: у нас она идёт сразу после ELF-заголовка, поэтому смещение равно размеру ELF-заголовка.
\end{itemize}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{pic/elf_header.png}
\end{minipage}
\begin{itemize}
\item Смещение таблицы разделов: в \texttt{man elf} говорят, что если этой таблицы нету, то смещение 0.
\item Процессоро-специфичные флаги: в \texttt{man elf} советуют 0.
\item Размер ELF-заголовка: фиксирован, можно посчитать (но в С есть удобный оператор \texttt{sizeof}, позволяющий получить размер структуры).
\item Размер таблицы программных заголовков: равен размеру программного заголовка (у нас всего один заголовок в таблице).
\item Число программных заголовков: у нас 1.
\item Размер таблицы разделов: у нас её нет, поэтому 0.
\item Число разделов: 0.
\item Номер раздела, содержащего таблицу имён разделов: у нас такого раздела нет, поэтому SHN\_UNDEF.
\end{itemize}
Программный заголовок описывает сегмент (в нашем случае сегмент кода) и состоит из следующих полей:
\\ \\
\begin{minipage}{0.65\textwidth}
\begin{itemize}
\item Тип сегмента: у нас PT\_LOAD (загрузочный).
\item Атрибуты сегмента: у нас PF\_X и PF\_R (доступен на исполнение и чтение).
\item Смещение сегмента от начала файла (равно размеру ELF-заголовка).
\item Адрес загрузки сегмента в виртуальную память
(пока непонятно, откуда взять этот адрес).
\item Адрес загрузки сегмента в физическую память
(поле зарезервировано для систем, в которых напрямую адресуется физическая память, у нас должно быть нулевым).
\item Размер сегмента в файле (равен размеру кода).
\item Размер сегмента в памяти (равен размеру кода).
\item Выравнивание.
Обычно программа ничего не знает о том, в какой области памяти располагается её код и данные.
Но иногда при работе с памятью хочется иметь какие-то гарантии:
например, что адрес выделенного блока памяти достаточно ``круглый'',
то есть несколько его младших битов --- нули.
Это позволяет использовать младшие биты адреса для своих нужд
(поскольку адрес однозначно определяется старшими, ненулевыми битами).
Наша программа не делает никаких подобных предположений, поэтому выравнивание нам не критично --- сойдёт 0.
\end{itemize}
\end{minipage}
\begin{minipage}{0.35\textwidth}
\centering
\includegraphics[width=\textwidth]{pic/elf_program_header.png}
\end{minipage}
\\ \\
Итак, осталось два непонятных поля: точка входа (из ELF-заголовка) и адрес загрузки сегмена кода в память (из программного заголовка).
В нашем случае оба поля совпадают: это адрес в памяти, на который загрузчик передаст управление, когда загрузит программу в память.
Но откуда взять это адрес: может ли он быть любым, или нужен какой-то стандартный?
Если у двух программ этот адрес совпадёт, они что, загрузятся в одну и ту же область памяти?
Чтобы ответить на эти вопросы, надо представлять, как программы загружаются в память.
\\ \\
\begin{minipage}{0.5\textwidth}
На файловой системе программа хранится в виде исполняемого файла (например, в формате ELF).
Это очень компактное представление: собственно код программы, инициализированные статические данные (у нас их нет) и вспомогательная информация.
\\ \\
Чтобы исполнить программу, загрузчик разворачивает её из компактного представления в \emph{образ} в памяти:
к коду и статическим данным добавляются стек и куча --- области памяти, в которых программа будет сохранять данные по мере исполнения.
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{pic/executable_image.png}
\end{minipage}
\\ \\
Размер стека и кучи не фиксирован (во время исполнения программа может выделять и освобождать память),
поэтому стек располагается в старших адресах памяти и растёт вниз,
а куча располагается в младших адресах (сразу после кода и даных) и растёт вверх.
При таком раскладе возникают два вопроса:
\begin{enumerate}
\item Что если одни части программы налезут на другие (например, стек на кучу)?
\item Что если надо одновременно загрузить в память несколько программ, как поделить память между ними?
\end{enumerate}
На оба эти вопроса ответ один --- виртуальная память.
\\ \\
Я уже немного рассказывала про виртуальную память: это логическое представление оперативной памяти
(обычно поверх другого логического представления --- сегментной модели памяти).
Каждой программе память представлется в виде виртуального адресного пространства.
В этом виртуальном адресном пространстве нет других программ (кроме ядра операционной системы и, возможно, динамических библиотек):
программе надо неслабо постараться, чтоб влезть в чужое адресное пространство.
Размер виртуального адресного пространства зависит от разрядности адреса: $2^N$ для архитектуры с N-битными адресами,
то есть 4 гигабайта для 32-битных адресов и 2 эксабайта для 64-битных адресов.
В архитектуре x86\_64 размер виртуального адресного пространства --- 256 терабайт (не все 64 бита адреса используются).
Это значительно превышает объём физической оперативной памяти (обычно несколько гигабайт).
\\ \\
Виртуальная память реализована на основе страничного механизма процессора:
адресное пространство разбито на страницы, которые по мере надобности загружаются в физическую память и выгружаются из неё.
Операционная система настраивает процессор и следит за отображением виртуальных страниц на физическую память.
Часть страниц виртуальной памяти привязана к реальной (оперативной или внешней) памяти (mapped), другие не привязаны (unmapped).
У каждой страницы есть права доступа на запись/чтение/исполнение (unmapped страницы недоступны ни на что).
Всё управление виртуальной памятью (выделение/освобождение/изменение прав доступа) программа вынуждена делать через операционую систему.
Если программа попытается нарушить права доступа к странице, операционная система поймает это нарушение и прервёт исполнение программы.
\\ \\
\begin{minipage}{0.35\textwidth}
Виртуальная память решает сразу несколько проблем.
Во-первых, разные части программы не налезут друг на друга: в виртуальном адресном пространстве между ними находятся ``буферные'' страницы (unmapped),
доступ к которым запрещён.
Во-вторых, загрузить несколько программ в память тоже не проблема: операционная система просто должна распределять
физическую память сразу между несколькими виртуальными пространствами.
При этом разные программы изолированы друг от друга.
В-третьих, для самих программ управление памятью значительно упрощается.
Из недостатков --- доступ к памяти становится ещё медленнее (особенно с учётом page fault'ов).
\end{minipage}
\begin{minipage}{0.65\textwidth}
\includegraphics[width=\textwidth]{pic/virtual_memory.png}
\end{minipage}
\\ \\
Я уже говорила (и нарисовала на картинке), что в адресном пространстве любой программы присутствует ядро операционной системы.
Зачем?
Во-первых, полезная программа не может без операционной системы:
выделение памяти, работа с файлами, межпроцессные взаимодействия --- всё требует системных вызовов.
Во-вторых, даже если сама программа не взаимодействует с операционной системой,
операционной системе может понадобиться провзаимодействовать с программой (напрмер, прервать её выполнение).
В-третьих, ядро операционной системы всё равно должно быть загружено в физическую память
(операционная система исполняется дольше всех программ, которые запущены из неё),
поэтому ничего не стоит замапить его в виртуальную память любой программы.
\\ \\
Кроме ядра операционной системы, программы часто используют библиотеки.
Следует различать \emph{статические} и \emph{динамические} библиотеки:
статические зашиваются прямо в программу (линкером),
а динамические отдельно грузятся в адресное пространство программы
(загрузчиком или самой программой во время исполнения).
Если программа написана на языке высокого уровня,
она почти наверняка будет слинкована со стандартной библиотекой этого языка.
\\ \\
Теперь можно вернуться к нашим недовычисленным полям: точке входа (\texttt{e\_entry}) и адресу загрузки сегмента кода (\texttt{p\_vaddr}).
Оба поля означают виртуальный адрес загрузки программы в память.
На этот адрес есть некоторые ограничения.
Первое ограничение --- по нулевому адресу грузить программу нельзя: там располагается одна из ``буферных'' областей виртуальной памяти.
Размер этой области в линуксе записан в файле /proc/sys/vm/mmap\_min\_addr.
Второе ограничение --- если внимательно прочитать \texttt{man elf}, то там написано, что \texttt{p\_vaddr} и \texttt{p\_offset}
должны быть сравнимы по модулю \texttt{p\_align} (давать одинаковый остаток от деления на \texttt{p\_align}).
С учётом обоих ограничений положим \texttt{e\_entry = p\_vaddr = mmap\_min\_addr + p\_offset}.
\\ \\
Ну что, всё? Почти. :)
Есть одна мелочь: наша программа не останавливается.
Скомпилировать-то мы её скомпилируем, потом загрузим в память и передадим на неё управление.
Программа радостно выполнится, и ...?
Никакой магии не произойдёт, начнут выполняться следующие за сегментом кода байты в памяти.
В лучшем случае эти байты окажутся в области памяти, недоступной на выполнение, и операционная система прибьёт программу сегфолтом.
В худшем случае может быть что угодно: начнёт исполняться неизвестный код
(который, может быть, только и ждал, пока какой-нибудь лапоть передаст на него управление).
\\ \\
Чтобы избежать этого позора, нам надо как-то завершить программу.
Вообще, это сделать довольно просто: достаточно вызвать какое-нибудь гадкое исключение.
Можно поделить на ноль,
можно обратиться по нулевому адресу,
можно попытаться исполнить невалидную команду процессора.
Во всех этих случаях операционная система гарантированно прибьёт программу (гарантированно --- это важно!).
Но есть способ завершить программу по-хорошему: сделать системный вызов \texttt{exit}.
Как видишь, даже наша простецкая программа не обошлась без системных вызовов.
\\ \\
В линуксе, чтобы сделать системный вызов, нужно положить номер вызова в EAX,
аргументы в другие регистры, и выполнить 80-е прерывание.
Номер \texttt{exit} --- 1, единственный аргумент --- код возврата в EBX.
Наша программа сохраняет результат выполнения в EAX, поэтому получаем такой код:
\begin{verbatim}
    mov ebx, eax
    mov eax, 1
    int 80
\end{verbatim}
Этот код должен идти сразу после кода программы.
Результат будет в коде возврата.
Посмотреть его можно командой \texttt{echo \$?}, выполнив её сразу после выполнения программы.
Правда, если результат не влазит в 1 байт, мы его не увидим:
старшие байты операционная система использует для своих нужд (выставляет там разные ошибки и статусы).
Кроме того, шелл может обрубить ещё половину кодов (сужая диапазон до $[0, 127]$).
Наконец, код возврата интерпретируется как беззнаковое число, поэтому отрицательные результаты мы тоже не увидим.
По-хорошему, нельзя возвращать 32-битный знаковый результат программы в коде возврата,
но в противном случае пришлось бы выводить результат на экран (конвертировать число в строку и делать системный вызов),
а я решила, что с тебя уже хватит. ;)
\\ \\
Вот теперь действительно всё.
Код генерации ELF-файла вот (файл gen\_elf64.h):
\lstinputlisting[language=C++]{example_lang_proc/gen_elf64.cpp}
А вот хедер компилятора (файл compiler.h):
\lstinputlisting[language=C++]{example_lang_proc/compiler.h}
Фух. Тяжёлая выдалась глава про компилятор.
Тут тебе и архитектура x86\_64 с её регистрами и набором команд,
и алгоритм компиляции AST в машинные инструкции, и формат ELF-64, и виртуальная память.
И главное, результат --- непортабельный компилятор для одной-единственной архитектуры.
В сравнении с полстраницей кода интерпретатора это как-то слишком.
Возникает вопрос: это всегда так просто писать интерпретаторы и так тяжело --- компиляторы?
Ответ --- в общем, да.
В компиляторе добавляется серьёзный этап --- генерация кода (причём хороший компилятор умеет компилировать сразу под много архитектур).
Зато как это прекрасно --- машинные инструкции, и всё такое. :D
\\ \\
Кроме того, помни, что в нашем примере полностью отсутствует оптимизация промежуточного представления.
С учётом оптимизаций разница в сложности компилятора и интерпретатора была бы не такой разительной.

\section{Виртуальная машина: байткод}
Байткод виртуальной машины будет представлять из себя
запись оригинальной программы в обратной польской нотации (Reverse Polish Notation, RPN).
Это постфиксная запись: сначала идут операнды, потом оператор.
Например, программа ``11 + (42 - 318) * 7'' в RPN выглядит так: ``11 42 318 - 7 * +''.
\\ \\
Виртуальная машина у нас будет \emph{стековая}: у неё будет память в виде стека
(для хранения промежуточных результатов), и она будет понимать команды:
\begin{itemize}
\item ADD --- достать из стека два верхних числа, сложить и запихнуть в стек
\item SUB --- достать из стека два верхних числа, вычесть и запихнуть в стек
\item MUL --- достать из стека два верхних числа, умножить и запихнуть в стек
\item DIV --- достать из стека два верхних числа, поделить и запихнуть в стек
\item NUMBER n --- запихнуть в стек число n.
\end{itemize}
Это, в общем-то, всё: очень простая виртуальная машина.
Вот компилятор программы в байткод (файл vm\_bytecode.cpp):
\lstinputlisting[language=C++]{example_lang_proc/vm_bytecode.cpp}
А вот его прототип (файл vm\_bytecode.h):
\lstinputlisting[language=C++]{example_lang_proc/vm_bytecode.h}

\section{Виртуальная машина: интерпретатор байткода}
Первый вариант реализации нашей виртуальной машины --- это интерпретатор байткода.
Для реализации стека в интерпретаторе используется \texttt{std::stack<int>}.
Больше никаких хитростей нет, интерпретатор просто в цикле читает и исполняет инструкции.
Вот код интерпретатора байткода (файл vm\_run.cpp):
\lstinputlisting[language=C++]{example_lang_proc/vm_run.cpp}
А вот его прототип (файл vm\_run.h):
\lstinputlisting[language=C++]{example_lang_proc/vm_run.h}

\section{Виртуальная машина: JIT-компилятор байткода}
Второй вариант реализации виртуальной машины чуть посложнее --- это JIT-компилятор байткода в нативный код x86\_64.
Но нам очень повезло: архитектура виртуальной машины --- это подмножество архитектуры x86\_64.
У x86\_64 есть готовый стек и инструкции работы с ним \texttt{PUSH} и \texttt{POP},
есть все необходимые арифметические инструкции.
Так что мы можем напрямую транслировать инструкции нашей виртуальной машины в инструкции x86\_64.
\\ \\
Ну хорошо, скомпилировать код проблем нет.
А вот как нам вызвать его, получить результат и вернуть управление в нашу программу?
В ассемблере для этого есть команды передачи управления, принимающие адрес в памяти как аргумент.
В C++ нет таких команд, и нам придётся немного схитрить: представить скомпилированный код
как обычную C++ функцию без аргументов, возвращающую \texttt{int}:
\begin{verbatim}
    int f ();
\end{verbatim}
Затем вызвать эту функцию и получить результат программы:
\begin{verbatim}
    int result = f ();
\end{verbatim}
Для воплощения этого плана нам понядобятся три вещи:
\begin{itemize}
\item кусок памяти, доступной на исполнение
\item указатель на функцию
\item конвенция вызова (calling convention)
\end{itemize}

\subsubsection{mmap}
Мы должны хранить скомпилированный код в непрерывном массиве памяти, и эта память должна быть доступна на исполнение.
Обычный \texttt{malloc} выделяет память, доступную на чтение и запись, но не на исполнение.
Если мы попытаемся исполнить память, выделенную \texttt{malloc}-ом, операционная система прибьет нашу программу сегфолтом.
Нам придётся выделять память по-другому: функцией \texttt{mmap}.
Эта функция отображает часть виртуального адресного пространства процесса на реальную память,
и принимает набор флагов, указывающих права доступа к выделенному куску памяти.

\subsubsection{Указатель на функцию}
В C++ есть такая штука как \emph{указатель на функцию}.
Например, если прототип функции выглядит вот так:
\begin{verbatim}
    int f ();
\end{verbatim}
То указатель на эту функцию можно создать так:
\begin{verbatim}
    int (* pf) () = f;
\end{verbatim}
То есть создаётся переменная \texttt{pf} типа \texttt{int (*) ()}, которая инициализируется значением \texttt{f}.
Это немного необычная запись: имя переменной засунуто в середину типа.
Вызывать функцию по указателю можно так же, как и по обычному имени.
Эти два вызова эквивалентны:
\begin{verbatim}
    f ();
    pf ();
\end{verbatim}
В данном примере указатель на функцию проинициализирован другой функцией с такой же сигнатурой.
Но ведь это не обязательно.
Если у нас есть адрес \texttt{addr} скомпилированного кода, мы можем сделать, чтобы указатель на функцию показывал на \texttt{addr}.
Для этого нам придётся привести тип \texttt{addr} к типу \texttt{int (*) ()}:
\begin{verbatim}
    int (* pf) () = (int (*) ()) addr;
\end{verbatim}
Вообще, преобразование типов --- гадкая и нехорошая вещь.
Его следует использовать с большой осторожностью, только в крайних случаях.
Оно позволяет делать страшные вещи (например, передавать управление на какой-то странный кусок памяти, чем мы и заняты).
Недаром в C++ есть целых четыре разных преобразования типов
(\texttt{static\_cast}, \texttt{const\_cast}, \texttt{dynamic\_cast}, \texttt{reinterpret\_cast})
в дополнение к старому C-шному \texttt{()}.
\\ \\
Теперь мы можем спокойно передавать управление на наш код:
\begin{verbatim}
    f ();
\end{verbatim}

\subsubsection{Конвенция вызова}
Мало просто просто состряпать указатель на функцию, показывающий на код в исполняемой памяти, и передать на него управление.
Надо ещё вернуться из этого кода и каким-то образом вернуть результат.
Чтобы это сделать, надо прежде всего задуматься: во что транслируется вызов функции в C++?
\\ \\
В этом всём нам поможет знание \emph{конвенции вызова}.
Конвенция вызова, или соглашение о вызове --- это договорённость о следующих особенностях реализации вызова подпрограммы:
\begin{itemize}
\item какую память использовать для передачи аргументов (регистры, стек, кучу, etc.)
\item в каком порядке передавать аргументы (в прямом, в обратном, etc.)
\item какую память использовать для возвращаемого значения (регистры, стек, кучу, etc.)
\item кто должен чистить память после иполнения подпрограммы (вызывающий, вызываемый, оба)
\item какие машинные команды использовать для передачи управления на подрограмму и возврата из неё
\end{itemize}
Конвенция вызова --- это часть бинарного интерфейса программы (Application Binary Interface, ABI),
то есть интерфейса взаимодействия программ на уровне машинного кода.
(Есть ещё API, Application Programming Interface --- это интерфейс взаимодействия программ на уровне исходного кода.)
\\ \\
В стандарте C++ конвенция вызова не определена --- она зависит от компилятора.
Обычно компилятор поддерживает несколько конвенций.
Мы будем использовать конвенцию cdecl (``C-declaration''):
аргументы передаются через стек, справа налево;
очистку стека производит вызывающая программа;
результат функции возвращается через регистр RAX.
Это конвенция-по-умолчанию в большинстве компиляторов C++, поэтому её не обязательно указывать.
\\ \\
Нашей функции надо сделать две вещи: положить в RAX возвращаемое значение вернуть управление.
Это делается двумя командами (учитывая, что результат выполнения программы у нас лежит на вершине стека):
\begin{verbatim}
    pop rax
    ret
\end{verbatim}
\bigskip
Итак, мы будем хранить скомпилированный код в структуре следующего вида (файл code.h):
\lstinputlisting[language=C++]{example_lang_proc/code.h}
У структуры есть конструктор, деструктор и два метода: \texttt{void save\_byte (unsigned char)} и \texttt{int exec ()}.
Вот их реализация (файл code.cpp):
\lstinputlisting[language=C++]{example_lang_proc/code.cpp}
Заметь, что вся память выделяется через \texttt{mmap} (освобождается через \texttt{munmap} соответственно),
а передача управления на скомпилированный код осуществляется при помощи указателя на функцию,
который грубой силой настроили на начало кода.
\\ \\
Сам JIT-компилятор выглядит проще некуда (файл vm\_jit.cpp):
\lstinputlisting[language=C++]{example_lang_proc/vm_jit.cpp}
Вот его прототип (файл vm\_jit.h):
\lstinputlisting[language=C++]{example_lang_proc/vm_jit.h}

\section{Итоги}
Что мы видели в этом примере?
\\ \\
Язык арифметических выражений.
Формальную грамматику для этого языка и алгоритм избавления от левой рекурсии.
Парсер, написанный по грамматике методом рекурсивного спуска.
AST.
Крайне простой интерпретатор, рекурсивно обходящий AST и вычисляющий результат.
Чуть более сложный компилятор для архитектуры x86\_64, генерирующий исполняемые файлы в формате ELF-64.
Стековую виртуальную машину, понимающую арифметические выражения в RPN,
и две её реализации: в виде простого интерпретатора и в виде JIT-компилятора для архитектуры x86\_64.
\\ \\
Чего мы не видели в этом примере?
\\ \\
Формально определённой семантики.
Типичной для языков программирования двухуровневой грамматики (лексической и синтаксической).
Оптимизаций.
Сложной взаимосвязи между разными фазами языкового процессора.
Наконец, из примера не чувствуется разительных достоинств и недостатков разных подходов:
захватывающей скорости скомпилированного машинного кода,
удивительной гибкости изменяющихся на лету интерпретируемых программ,
хитро адаптирующихся к жизни JIT-компиляторов
и платформенно-независимого байткода виртуальных машин.
Что действительно бросилось в глаза --- так это трудность портирования компилятора на x86\_64.
\\ \\
Ну что --- на этом всё. :)
Спасибо за (возможно, напрасно) потраченное внимание и время. :)

\end{document}
